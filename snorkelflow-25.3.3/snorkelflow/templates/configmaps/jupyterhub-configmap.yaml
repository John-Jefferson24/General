{{- $images := .Files.Get "images.json" | fromJson }}
{{- if .Values.services.jupyterhub.enabled }}
apiVersion: v1
data:
  jupyterhub: jupyterhub
  jupyterhubconfig.py: |
    # load the config object (satisfies linters)
    # Based on: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/4.0.0/jupyterhub/files/hub/jupyterhub_config.py
    c = get_config()  # noqa

    import glob
    import os
    import re
    import sys

    from jupyterhub.utils import url_path_join
    from kubernetes_asyncio import client
    from kubespawner.spawner import KubeSpawner
    from tornado.httpclient import AsyncHTTPClient

    # Make sure that modules placed in the same directory as the jupyterhub config are added to the pythonpath
    configuration_directory = os.path.dirname(os.path.realpath(__file__))
    sys.path.insert(0, configuration_directory)

    from z2jh import (
        get_config,
        get_name,
        get_name_env,
        get_secret_value,
        set_config_if_not_none,
    )


    def camelCaseify(s):
        """convert snake_case to camelCase

        For the common case where some_value is set from someValue
        so we don't have to specify the name twice.
        """
        return re.sub(r"_([a-z])", lambda m: m.group(1).upper(), s)


    # Configure JupyterHub to use the curl backend for making HTTP requests,
    # rather than the pure-python implementations. The default one starts
    # being too slow to make a large number of requests to the proxy API
    # at the rate required.
    AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")

    c.JupyterHub.spawner_class = "kubespawner.KubeSpawner"

    # Connect to a proxy running in a different pod. Note that *_SERVICE_*
    # environment variables are set by Kubernetes for Services
    c.ConfigurableHTTPProxy.api_url = (
        f'http://{get_name("proxy-api")}:{get_name("proxy-api-port")}'
    )
    c.ConfigurableHTTPProxy.should_start = False

    # Do not shut down user pods when hub is restarted
    c.JupyterHub.cleanup_servers = False

    # Check that the proxy has routes appropriately setup
    c.JupyterHub.last_activity_interval = 60

    # Don't wait at all before redirecting a spawning user to the progress page
    c.JupyterHub.tornado_settings = {
        "headers": {
            "Content-Security-Policy": "script-src * data: blob: 'unsafe-inline' 'unsafe-eval';"
        },
        "slow_spawn_timeout": 0,
    }


    # configure the hub db connection
    db_type = get_config("hub.db.type")
    if db_type == "sqlite-pvc":
        c.JupyterHub.db_url = "sqlite:///jupyterhub.sqlite"
    elif db_type == "sqlite-memory":
        c.JupyterHub.db_url = "sqlite://"
    else:
        set_config_if_not_none(c.JupyterHub, "db_url", "hub.db.url")
    db_password = get_secret_value("hub.db.password", None)
    if db_password is not None:
        if db_type == "mysql":
            os.environ["MYSQL_PWD"] = db_password
        elif db_type == "postgres":
            os.environ["PGPASSWORD"] = db_password
        else:
            print(f"Warning: hub.db.password is ignored for hub.db.type={db_type}")


    # c.JupyterHub configuration from Helm chart's configmap
    for trait, cfg_key in (
        ("concurrent_spawn_limit", None),
        ("active_server_limit", None),
        ("base_url", None),
        ("allow_named_servers", None),
        ("named_server_limit_per_user", None),
        ("authenticate_prometheus", None),
        ("redirect_to_server", None),
        ("shutdown_on_logout", None),
        ("template_paths", None),
        ("template_vars", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.JupyterHub, trait, "hub." + cfg_key)

    # hub_bind_url configures what the JupyterHub process within the hub pod's
    # container should listen to.
    hub_container_port = 8081
    c.JupyterHub.hub_bind_url = f"http://:{hub_container_port}"

    # hub_connect_url is the URL for connecting to the hub for use by external
    # JupyterHub services such as the proxy. Note that *_SERVICE_* environment
    # variables are set by Kubernetes for Services.
    c.JupyterHub.hub_connect_url = (
        f'http://{get_name("jupyterhub")}:{get_name_env("jupyterhub", "_SERVICE_PORT")}'
    )

    # implement common labels
    # This mimics the jupyterhub.commonLabels helper, but declares managed-by to
    # kubespawner instead of helm.
    #
    # The labels app and release are old labels enabled to be deleted in z2jh 5, but
    # for now retained to avoid a breaking change in z2jh 4 that would force user
    # server restarts. Restarts would be required because NetworkPolicy resources
    # must select old/new pods with labels that then needs to be seen on both
    # old/new pods, and we want these resources to keep functioning for old/new user
    # server pods during an upgrade.
    #
    common_labels = c.KubeSpawner.common_labels = {}
    common_labels["app.kubernetes.io/name"] = common_labels["app"] = get_config(
        "nameOverride",
        default=get_config("Chart.Name", "jupyterhub"),
    )
    release = get_config("Release.Name")
    if release:
        common_labels["app.kubernetes.io/instance"] = common_labels["release"] = release
    chart_name = get_config("Chart.Name")
    chart_version = get_config("Chart.Version")
    if chart_name and chart_version:
        common_labels["helm.sh/chart"] = common_labels["chart"] = (
            f"{chart_name}-{chart_version.replace('+', '_')}"
        )
    common_labels["app.kubernetes.io/managed-by"] = "kubespawner"

    c.KubeSpawner.namespace = os.environ.get("POD_NAMESPACE", "default")

    # Max number of consecutive failures before the Hub restarts itself
    # requires jupyterhub 0.9.2
    set_config_if_not_none(
        c.Spawner,
        "consecutive_failure_limit",
        "hub.consecutiveFailureLimit",
    )

    for trait, cfg_key in (
        ("pod_name_template", None),
        ("start_timeout", None),
        ("image_pull_policy", "image.pullPolicy"),
        # ('image_pull_secrets', 'image.pullSecrets'), # Managed manually below
        ("events_enabled", "events"),
        ("extra_labels", None),
        ("extra_annotations", None),
        # ("allow_privilege_escalation", None), # Managed manually below
        ("uid", None),
        ("fs_gid", None),
        ("service_account", "serviceAccountName"),
        ("storage_extra_labels", "storage.extraLabels"),
        # ("tolerations", "extraTolerations"), # Managed manually below
        ("node_selector", None),
        ("node_affinity_required", "extraNodeAffinity.required"),
        ("node_affinity_preferred", "extraNodeAffinity.preferred"),
        ("pod_affinity_required", "extraPodAffinity.required"),
        ("pod_affinity_preferred", "extraPodAffinity.preferred"),
        ("pod_anti_affinity_required", "extraPodAntiAffinity.required"),
        ("pod_anti_affinity_preferred", "extraPodAntiAffinity.preferred"),
        ("lifecycle_hooks", None),
        ("init_containers", None),
        ("extra_containers", None),
        ("mem_limit", "memory.limit"),
        ("mem_guarantee", "memory.guarantee"),
        ("cpu_limit", "cpu.limit"),
        ("cpu_guarantee", "cpu.guarantee"),
        ("extra_resource_limits", "extraResource.limits"),
        ("extra_resource_guarantees", "extraResource.guarantees"),
        ("environment", "extraEnv"),
        ("profile_list", None),
        ("extra_pod_config", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.KubeSpawner, trait, "singleuser." + cfg_key)

    image = get_config("singleuser.image.name")
    if image:
        tag = get_config("singleuser.image.tag")
        if tag:
            image = f"{image}:{tag}"

        c.KubeSpawner.image = image

    # allow_privilege_escalation defaults to False in KubeSpawner 2+. Since its a
    # property where None, False, and True all are valid values that users of the
    # Helm chart may want to set, we can't use the set_config_if_not_none helper
    # function as someone may want to override the default False value to None.
    #
    c.KubeSpawner.allow_privilege_escalation = get_config(
        "singleuser.allowPrivilegeEscalation"
    )

    # Combine imagePullSecret.create (single), imagePullSecrets (list), and
    # singleuser.image.pullSecrets (list).
    image_pull_secrets = []
    if get_config("imagePullSecret.automaticReferenceInjection") and get_config(
        "imagePullSecret.create"
    ):
        image_pull_secrets.append(get_name("image-pull-secret"))
    if get_config("imagePullSecrets"):
        image_pull_secrets.extend(get_config("imagePullSecrets"))
    if get_config("singleuser.image.pullSecrets"):
        image_pull_secrets.extend(get_config("singleuser.image.pullSecrets"))
    if image_pull_secrets:
        c.KubeSpawner.image_pull_secrets = image_pull_secrets

    # scheduling:
    if get_config("scheduling.userScheduler.enabled"):
        c.KubeSpawner.scheduler_name = get_name("user-scheduler")
    if get_config("scheduling.podPriority.enabled"):
        c.KubeSpawner.priority_class_name = get_name("priority")

    # add node-purpose affinity
    match_node_purpose = get_config("scheduling.userPods.nodeAffinity.matchNodePurpose")
    if match_node_purpose:
        node_selector = dict(
            matchExpressions=[
                dict(
                    key="hub.jupyter.org/node-purpose",
                    operator="In",
                    values=["user"],
                )
            ],
        )
        if match_node_purpose == "prefer":
            c.KubeSpawner.node_affinity_preferred.append(
                dict(
                    weight=100,
                    preference=node_selector,
                ),
            )
        elif match_node_purpose == "require":
            c.KubeSpawner.node_affinity_required.append(node_selector)
        elif match_node_purpose == "ignore":
            pass
        else:
            raise ValueError(
                f"Unrecognized value for matchNodePurpose: {match_node_purpose}"
            )

    # Combine the common tolerations for user pods with singleuser tolerations
    scheduling_user_pods_tolerations = get_config("scheduling.userPods.tolerations", [])
    singleuser_extra_tolerations = get_config("singleuser.extraTolerations", [])
    tolerations = scheduling_user_pods_tolerations + singleuser_extra_tolerations
    if tolerations:
        c.KubeSpawner.tolerations = tolerations

    # Configure dynamically provisioning pvc
    storage_type = get_config("singleuser.storage.type")
    if storage_type == "dynamic":
        pvc_name_template = get_config("singleuser.storage.dynamic.pvcNameTemplate")
        if pvc_name_template:
            c.KubeSpawner.pvc_name_template = pvc_name_template
        volume_name_template = get_config("singleuser.storage.dynamic.volumeNameTemplate")
        c.KubeSpawner.storage_pvc_ensure = True
        set_config_if_not_none(
            c.KubeSpawner, "storage_class", "singleuser.storage.dynamic.storageClass"
        )
        set_config_if_not_none(
            c.KubeSpawner,
            "storage_access_modes",
            "singleuser.storage.dynamic.storageAccessModes",
        )
        set_config_if_not_none(
            c.KubeSpawner, "storage_capacity", "singleuser.storage.capacity"
        )

        # Add volumes to singleuser pods
        c.KubeSpawner.volumes = [
            {
                "name": volume_name_template,
                "persistentVolumeClaim": {"claimName": "{pvc_name}"},
            }
        ]
        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": volume_name_template,
                "subPath": get_config("singleuser.storage.dynamic.subPath"),
            }
        ]
    elif storage_type == "static":
        pvc_claim_name = get_config("singleuser.storage.static.pvcName")
        c.KubeSpawner.volumes = [
            {"name": "home", "persistentVolumeClaim": {"claimName": pvc_claim_name}}
        ]

        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": "home",
                "subPath": get_config("singleuser.storage.static.subPath"),
            }
        ]

    # Inject singleuser.extraFiles as volumes and volumeMounts with data loaded from
    # the dedicated k8s Secret prepared to hold the extraFiles actual content.
    extra_files = get_config("singleuser.extraFiles", {})
    if extra_files:
        volume = {
            "name": "files",
        }
        items = []
        for file_key, file_details in extra_files.items():
            # Each item is a mapping of a key in the k8s Secret to a path in this
            # abstract volume, the goal is to enable us to set the mode /
            # permissions only though so we don't change the mapping.
            item = {
                "key": file_key,
                "path": file_key,
            }
            if "mode" in file_details:
                item["mode"] = file_details["mode"]
            items.append(item)
        volume["secret"] = {
            "secretName": get_name("singleuser"),
            "items": items,
        }
        c.KubeSpawner.volumes.append(volume)

        volume_mounts = []
        for file_key, file_details in extra_files.items():
            volume_mounts.append(
                {
                    "mountPath": file_details["mountPath"],
                    "subPath": file_key,
                    "name": "files",
                }
            )
        c.KubeSpawner.volume_mounts.extend(volume_mounts)

    # Inject extraVolumes / extraVolumeMounts
    c.KubeSpawner.volumes.extend(get_config("singleuser.storage.extraVolumes", []))
    c.KubeSpawner.volume_mounts.extend(
        get_config("singleuser.storage.extraVolumeMounts", [])
    )

    c.JupyterHub.services = []
    c.JupyterHub.load_roles = []

    # jupyterhub-idle-culler's permissions are scoped to what it needs only, see
    # https://github.com/jupyterhub/jupyterhub-idle-culler#permissions.
    #
    if get_config("cull.enabled", False):
        jupyterhub_idle_culler_role = {
            "name": "jupyterhub-idle-culler",
            "scopes": [
                "list:users",
                "read:users:activity",
                "read:servers",
                "delete:servers",
                # "admin:users", # dynamically added if --cull-users is passed
            ],
            # assign the role to a jupyterhub service, so it gains these permissions
            "services": ["jupyterhub-idle-culler"],
        }

        cull_cmd = ["python3", "-m", "jupyterhub_idle_culler"]
        base_url = c.JupyterHub.get("base_url", "/")
        cull_cmd.append("--url=http://localhost:8081" + url_path_join(base_url, "hub/api"))

        cull_timeout = get_config("cull.timeout")
        if cull_timeout:
            cull_cmd.append(f"--timeout={cull_timeout}")

        cull_every = get_config("cull.every")
        if cull_every:
            cull_cmd.append(f"--cull-every={cull_every}")

        cull_concurrency = get_config("cull.concurrency")
        if cull_concurrency:
            cull_cmd.append(f"--concurrency={cull_concurrency}")

        if get_config("cull.users"):
            cull_cmd.append("--cull-users")
            jupyterhub_idle_culler_role["scopes"].append("admin:users")

        if not get_config("cull.adminUsers"):
            cull_cmd.append("--cull-admin-users=false")

        if get_config("cull.removeNamedServers"):
            cull_cmd.append("--remove-named-servers")

        cull_max_age = get_config("cull.maxAge")
        if cull_max_age:
            cull_cmd.append(f"--max-age={cull_max_age}")

        c.JupyterHub.services.append(
            {
                "name": "jupyterhub-idle-culler",
                "command": cull_cmd,
            }
        )
        c.JupyterHub.load_roles.append(jupyterhub_idle_culler_role)

    for key, service in get_config("hub.services", {}).items():
        # c.JupyterHub.services is a list of dicts, but
        # hub.services is a dict of dicts to make the config mergable
        service.setdefault("name", key)

        # As the api_token could be exposed in hub.existingSecret, we need to read
        # it it from there or fall back to the chart managed k8s Secret's value.
        service.pop("apiToken", None)
        service["api_token"] = get_secret_value(f"hub.services.{key}.apiToken")

        c.JupyterHub.services.append(service)

    for key, role in get_config("hub.loadRoles", {}).items():
        # c.JupyterHub.load_roles is a list of dicts, but
        # hub.loadRoles is a dict of dicts to make the config mergable
        role.setdefault("name", key)

        c.JupyterHub.load_roles.append(role)

    # respect explicit null command (distinct from unspecified)
    # this avoids relying on KubeSpawner.cmd's default being None
    _unspecified = object()
    specified_cmd = get_config("singleuser.cmd", _unspecified)
    if specified_cmd is not _unspecified:
        c.Spawner.cmd = specified_cmd

    set_config_if_not_none(c.Spawner, "default_url", "singleuser.defaultUrl")

    cloud_metadata = get_config("singleuser.cloudMetadata")

    if cloud_metadata.get("blockWithIptables") is True:
        # Use iptables to block access to cloud metadata by default
        network_tools_image_name = get_config("singleuser.networkTools.image.name")
        network_tools_image_tag = get_config("singleuser.networkTools.image.tag")
        network_tools_resources = get_config("singleuser.networkTools.resources")
        ip = cloud_metadata["ip"]
        ip_block_container = client.V1Container(
            name="block-cloud-metadata",
            image=f"{network_tools_image_name}:{network_tools_image_tag}",
            command=[
                "iptables",
                "--append",
                "OUTPUT",
                "--protocol",
                "tcp",
                "--destination",
                ip,
                "--destination-port",
                "80",
                "--jump",
                "DROP",
            ],
            security_context=client.V1SecurityContext(
                privileged=False,
                run_as_user=0,
                capabilities=client.V1Capabilities(add=["NET_ADMIN"]),
            ),
            resources=network_tools_resources,
        )

        c.KubeSpawner.init_containers.append(ip_block_container)


    if get_config("debug.enabled", False):
        c.JupyterHub.log_level = "DEBUG"
        c.Spawner.debug = True

    # load potentially seeded secrets
    #
    # NOTE: ConfigurableHTTPProxy.auth_token is set through an environment variable
    #       that is set using the chart managed secret.
    c.JupyterHub.cookie_secret = get_secret_value("hub.config.JupyterHub.cookie_secret")
    # NOTE: CryptKeeper.keys should be a list of strings, but we have encoded as a
    #       single string joined with ; in the k8s Secret.
    #
    c.CryptKeeper.keys = get_secret_value("hub.config.CryptKeeper.keys").split(";")

    # load hub.config values, except potentially seeded secrets already loaded
    for app, cfg in get_config("hub.config", {}).items():
        if app == "JupyterHub":
            cfg.pop("proxy_auth_token", None)
            cfg.pop("cookie_secret", None)
            cfg.pop("services", None)
        elif app == "ConfigurableHTTPProxy":
            cfg.pop("auth_token", None)
        elif app == "CryptKeeper":
            cfg.pop("keys", None)
        c[app].update(cfg)

    # load /usr/local/etc/jupyterhub/jupyterhub_config.d config files
    config_dir = "/usr/local/etc/jupyterhub/jupyterhub_config.d"
    if os.path.isdir(config_dir):
        for file_path in sorted(glob.glob(f"{config_dir}/*.py")):
            file_name = os.path.basename(file_path)
            print(f"Loading {config_dir} config: {file_name}")
            with open(file_path) as f:
                file_content = f.read()
            # compiling makes debugging easier: https://stackoverflow.com/a/437857
            exec(compile(source=file_content, filename=file_name, mode="exec"))

    # execute hub.extraConfig entries
    for key, config_py in sorted(get_config("hub.extraConfig", {}).items()):
        print(f"Loading extra config: {key}")
        exec(config_py)

    #########################################
    ##### Snorkel Flow Specific Changes #####
    #########################################

    c.Spawner.default_url = "/lab"

    # Automatically start users server after they login.
    # This settings automatically starts a users notebook server x number of seconds after
    # they have reached the launch server page. Because this page is iframed, a user should
    # normally never hit the launch server page after logging in, if they wait at least 1 second
    # before clicking the notebook page.
    c.JupyterHub.implicit_spawn_seconds = 1  # noqa: F821

    # Increase timeout for our large images pull and complicated startup process
    c.KubeSpawner.start_timeout = 60 * 5
    c.KubeSpawner.http_timeout = 60 * 2

    # Default to old naming scheme
    # https://jupyterhub-kubespawner.readthedocs.io/en/latest/templates.html#templates-upgrading-from-less-than-7
    c.KubeSpawner.slug_scheme = "escape"

    # Create a k8s service to use its service IP address instead of pod IP address
    # This is essential for the strict mTLS mode with Istio because Istio
    # configures sidecar proxies by looking up the k8s services.
    c.KubeSpawner.services_enabled = True

    # Apply the user api key secret to env before spanning
    def secret_hook(spawner: KubeSpawner):
        volume_name = "snorkel-api-key"
        mount_path = "/var/run/snorkelflow/api"
        spawner.volumes.append(
            {
                "name": volume_name,
                "secret": {
                    "secretName": spawner._expand_user_properties(
                        "snorkel-api-key-{escaped_username}"
                    ),
                },
            }
        )
        spawner.volume_mounts.append(
            {
                "name": volume_name,
                "mountPath": mount_path,
                "readOnly": True,
            }
        )
        spawner.environment["SNORKELFLOW_API_KEY_FILE"] = f"{mount_path}/key"
{{- if .Values.services.jupyterhub.secretsFromFile }}
        spawner.volumes.append(
            {
                "name": "minio-secret-volume",
                "secret": {
                    "secretName": "minio-secret"
                },
            },
        )
        spawner.volume_mounts.append(
            {
                "name": "minio-secret-volume",
                "mountPath": "/etc/secrets/minio-secret",
                "readOnly": True,
            },
        )
{{- if .Values.services.jupyterhub.singleUserNotebook.minio_user_dir_only }}
        spawner.environment["MINIO_ACCESS_KEY_FILE"] = "/etc/secrets/minio-secret/user_dir_only_user_name"
        spawner.environment["MINIO_SECRET_KEY_FILE"] = "/etc/secrets/minio-secret/user_dir_only_user_password"
{{- else }}
        spawner.environment["MINIO_ACCESS_KEY_FILE"] = "/etc/secrets/minio-secret/minio_access_key"
        spawner.environment["MINIO_SECRET_KEY_FILE"] = "/etc/secrets/minio-secret/minio_secret_key"
{{- end }}
{{- end }}

    c.Spawner.pre_spawn_hook = secret_hook

{{- if .Values.services.jupyterhub.secretsFromFile}}
    # If secretsFromFile is true, proxy_auth_token and client_secret are set in the jupyterhub_config.py
    # instead of env variables. See the following links for the more info about the fields:
    # * https://jupyterhub.readthedocs.io/en/0.7.2/getting-started.html#proxy-authentication-token
    # * https://oauthenticator.readthedocs.io/en/latest/reference/api/gen/oauthenticator.generic.html#oauthenticator.generic.GenericOAuthenticator.client_secret
    def get_file_secret(env_var_name: str) -> Optional[str]:
        path = os.getenv(env_var_name, None)
        if path is not None and os.path.exists(path):
            with open(path) as fd:
                return fd.read().strip()
        return None

    c.JupyterHub.proxy_auth_token = get_file_secret("CONFIGPROXY_AUTH_TOKEN_FILE")
    c.GenericOAuthenticator.client_secret = get_file_secret("OAUTH_CLIENT_SECRET_FILE")
{{- end}}
  proxy: proxy
  proxy-api: jupyterhub-proxy
  proxy-api-port: "8001"
  proxy-http: proxy-http
  proxy-public: jupyterhub-proxy
  proxy-public-manual-tls: proxy-public-manual-tls
  proxy-public-port: "8000"
  proxy-public-tls: proxy-public-tls-acme
  singleuser: jupyterhub-singleuser-shared-secret
  user-scheduler: jupyterhub-user-scheduler
  values.yaml: |-
    Chart:
      Name: jupyterhub
      Version: 4.0.0
    Release:
      Name: jupyterhub
      Namespace: {{ .Values.projectName }}
      Service: Helm
    cull:
      adminUsers: true
      concurrency: 10
      enabled: true
      every: 600
      maxAge: 0
      removeNamedServers: false
      timeout: 14400
      users: false
    custom: {}
    enabled: null
    fullnameOverride: ''
    global:
      safeToShowValues: false
    hub:
      activeServerLimit: null
      allowNamedServers: false
      annotations: {}
      args: []
      authenticatePrometheus: null
{{- if .Values.traffic.basePath }}
      baseUrl: {{.Values.traffic.basePath}}/jupyterhub
{{- else }}
      baseUrl: /jupyterhub
{{- end }}
      command: []
      concurrentSpawnLimit: 64
      config:
        Authenticator:
          auto_login: true
        GenericOAuthenticator:
          allow_all: false
          allowed_scopes:
          - openid
          - notebook_allowed
{{- if .Values.traffic.basePath }}
          authorize_url: {{ .Values.traffic.basePath }}/api/sso/oidc-provider/authorization
{{- else }}
          authorize_url: /api/sso/oidc-provider/authorization
{{- end }}
          basic_auth: false
          client_id: jupyterhub
{{- if .Values.traffic.basePath }}
          oauth_callback_url: {{ .Values.traffic.basePath }}/jupyterhub/hub/oauth_callback
{{- else }}
          oauth_callback_url: /jupyterhub/hub/oauth_callback
{{- end }}
          scope:
          - openid
          - notebook_allowed
          token_url: http://tdm-api:8686/oidcop/token
          userdata_url: http://tdm-api:8686/oidcop/userinfo
          username_claim: sub
        JupyterHub:
          admin_access: true
          authenticator_class: generic-oauth
      consecutiveFailureLimit: 5
      containerSecurityContext:
        allowPrivilegeEscalation: false
        runAsGroup: 1000
        runAsUser: 1000
      cookieSecret: null
      db:
        password: null
        pvc:
          accessModes:
          - ReadWriteOnce
          annotations: {}
          selector: {}
          storage: 1Gi
          storageClassName: null
          subPath: null
        type: sqlite-pvc
        upgrade: null
        url: null
      deploymentStrategy:
        type: Recreate
      existingSecret: null
      extraConfig: {}
      extraContainers: []
      extraEnv: {}
      extraFiles: {}
      extraPodSpec: {}
      extraVolumeMounts: []
      extraVolumes: []
      image:
        name: {{ template "snorkelflow.registryPrefix" . }}snorkelai/jupyterhub
        pullPolicy: null
        pullSecrets: []
        tag: {{ (split ":" (get $images.custom_images "jupyterhub"))._1 }}
      initContainers: []
      labels: {}
      lifecycle: {}
      livenessProbe:
        enabled: true
        failureThreshold: 30
        initialDelaySeconds: 300
        periodSeconds: 10
        timeoutSeconds: 3
      loadRoles: {}
      namedServerLimitPerUser: null
      networkPolicy:
        allowedIngressPorts: []
        egress: []
        egressAllowRules:
          cloudMetadataServer: true
          dnsPortsCloudMetadataServer: true
          dnsPortsKubeSystemNamespace: true
          dnsPortsPrivateIPs: true
          nonPrivateIPs: true
          privateIPs: true
        enabled: true
        ingress: []
        interNamespaceAccessLabels: ignore
      nodeSelector: {}
      pdb:
        enabled: false
        maxUnavailable: null
        minAvailable: 1
      podSecurityContext:
        fsGroup: 1000
      readinessProbe:
        enabled: true
        failureThreshold: 1000
        initialDelaySeconds: 0
        periodSeconds: 2
        timeoutSeconds: 1
      redirectToServer: null
      resources: {}
      revisionHistoryLimit: null
      service:
        annotations: {}
        extraPorts: []
        loadBalancerIP: null
        ports:
          nodePort: null
        type: ClusterIP
      serviceAccount:
        annotations: {}
        create: true
        name: null
      services: {}
      shutdownOnLogout: true
      templatePaths: []
      templateVars: {}
      tolerations: []
    imagePullSecret:
      automaticReferenceInjection: true
      create: false
      email: null
      password: null
      registry: null
      username: null
    imagePullSecrets: []
    ingress:
      annotations: {}
      enabled: false
      hosts: []
      ingressClassName: null
      pathSuffix: null
      pathType: Prefix
      tls: []
    prePuller:
      annotations: {}
      containerSecurityContext:
        allowPrivilegeEscalation: false
        runAsGroup: 65534
        runAsUser: 65534
      continuous:
        enabled: false
      extraImages: {}
      extraTolerations: []
      hook:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 65534
          runAsUser: 65534
        enabled: false
        image:
          name: quay.io/jupyterhub/k8s-image-awaiter
          pullPolicy: null
          pullSecrets: []
          tag: 1.2.0
        nodeSelector: {}
        podSchedulingWaitDuration: 10
        pullOnlyOnChanges: true
        resources: {}
        serviceAccount:
          annotations: {}
          create: true
          name: null
        tolerations: []
      labels: {}
      pause:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 65534
          runAsUser: 65534
        image:
          name: registry.k8s.io/pause
          pullPolicy: null
          pullSecrets: []
          tag: '3.9'
      pullProfileListImages: true
      resources: {}
      revisionHistoryLimit: null
    proxy:
      annotations: {}
      chp:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 65534
          runAsUser: 65534
        defaultTarget: null
        errorTarget: null
        extraCommandLineFlags: []
        extraEnv: {}
        extraPodSpec: {}
        image:
          name: {{ template "snorkelflow.registryPrefix" . }}snorkelai/jupyterhub-proxy
          pullPolicy: null
          pullSecrets: []
          tag: {{ (split ":" (get $images.custom_images "jupyterhub-proxy"))._1 }}
        livenessProbe:
          enabled: true
          failureThreshold: 30
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 3
        networkPolicy:
          allowedIngressPorts:
          - http
          - https
          egress: []
          egressAllowRules:
            cloudMetadataServer: true
            dnsPortsCloudMetadataServer: true
            dnsPortsKubeSystemNamespace: true
            dnsPortsPrivateIPs: true
            nonPrivateIPs: true
            privateIPs: true
          enabled: true
          ingress: []
          interNamespaceAccessLabels: ignore
        nodeSelector: {}
        pdb:
          enabled: false
          maxUnavailable: null
          minAvailable: 1
        readinessProbe:
          enabled: true
          failureThreshold: 1000
          initialDelaySeconds: 0
          periodSeconds: 2
          timeoutSeconds: 1
        resources: {}
        revisionHistoryLimit: null
        tolerations: []
      deploymentStrategy:
        rollingUpdate: null
        type: Recreate
      https:
        enabled: false
        hosts: []
        letsencrypt:
          acmeServer: https://acme-v02.api.letsencrypt.org/directory
          contactEmail: null
        manual:
          cert: null
          key: null
        secret:
          crt: tls.crt
          key: tls.key
          name: null
        type: letsencrypt
      labels: {}
      secretSync:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 65534
          runAsUser: 65534
        image:
          name: quay.io/jupyterhub/k8s-secret-sync
          pullPolicy: null
          pullSecrets: []
          tag: 1.2.0
        resources: {}
      secretToken: null
      service:
        annotations: {}
        disableHttpPort: false
        extraPorts: []
        labels: {}
        loadBalancerIP: null
        loadBalancerSourceRanges: []
        nodePorts:
          http: null
          https: null
        type: LoadBalancer
      traefik:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 65534
          runAsUser: 65534
        extraDynamicConfig: {}
        extraEnv: {}
        extraInitContainers: []
        extraPodSpec: {}
        extraPorts: []
        extraStaticConfig: {}
        extraVolumeMounts: []
        extraVolumes: []
        hsts:
          includeSubdomains: false
          maxAge: 15724800
          preload: false
        image:
          name: traefik
          pullPolicy: null
          pullSecrets: []
          tag: v2.11.0
        labels: {}
        networkPolicy:
          allowedIngressPorts:
          - http
          - https
          egress: []
          egressAllowRules:
            cloudMetadataServer: true
            dnsPortsCloudMetadataServer: true
            dnsPortsKubeSystemNamespace: true
            dnsPortsPrivateIPs: true
            nonPrivateIPs: true
            privateIPs: true
          enabled: true
          ingress: []
          interNamespaceAccessLabels: ignore
        nodeSelector: {}
        pdb:
          enabled: false
          maxUnavailable: null
          minAvailable: 1
        resources: {}
        serviceAccount:
          annotations: {}
          create: true
          name: null
        tolerations: []
    rbac:
      create: true
    scheduling:
      corePods:
        nodeAffinity:
          matchNodePurpose: prefer
        tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      podPriority:
        defaultPriority: 0
        enabled: false
        globalDefault: false
        userPlaceholderPriority: -10
      userPlaceholder:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 65534
          runAsUser: 65534
        enabled: true
        image:
          name: registry.k8s.io/pause
          pullPolicy: null
          pullSecrets: []
          tag: '3.9'
        replicas: 0
        resources: {}
      userPods:
        nodeAffinity:
          matchNodePurpose: prefer
      userScheduler:
        annotations: {}
        containerSecurityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 65534
          runAsUser: 65534
        enabled: false
        extraPodSpec: {}
        image:
          name: registry.k8s.io/kube-scheduler
          pullPolicy: null
          pullSecrets: []
          tag: v1.28.8
        labels: {}
        logLevel: 4
        nodeSelector: {}
        pdb:
          enabled: true
          maxUnavailable: 1
          minAvailable: null
        pluginConfig:
        - args:
            scoringStrategy:
              resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              type: MostAllocated
          name: NodeResourcesFit
        plugins:
          score:
            disabled:
            - name: NodeResourcesBalancedAllocation
            - name: NodeAffinity
            - name: InterPodAffinity
            - name: NodeResourcesFit
            - name: ImageLocality
            enabled:
            - name: NodeAffinity
              weight: 14631
            - name: InterPodAffinity
              weight: 1331
            - name: NodeResourcesFit
              weight: 121
            - name: ImageLocality
              weight: 11
        replicas: 2
        resources: {}
        revisionHistoryLimit: null
        serviceAccount:
          annotations: {}
          create: true
          name: null
        tolerations: []
    singleuser:
      allowPrivilegeEscalation: false
      args: []
      cloudMetadata:
        blockWithIptables: false
        ip: 169.254.169.254
      cmd: singleuser-notebook
      cpu:
        guarantee: {{ .Values.services.jupyterhub.singleUserNotebook.resources.cpu_guarantee }}
        limit: {{ .Values.services.jupyterhub.singleUserNotebook.resources.cpu_limit }}
      defaultUrl: null
      events: true
      extraAnnotations: {}
      extraContainers: []
      extraEnv:
        HOME: /data
{{- if and (and .Values.gpu.enabled .Values.gpu.gpu_config) .Values.services.jupyterhub.singleUserNotebook.gpu }}
        IS_GPU_ENABLED: '1'
{{- else }}
        IS_GPU_ENABLED: '0'
{{- end }}
{{- if not .Values.services.jupyterhub.secretsFromFile }}
{{- /* If secretsFromFile is true, secrets keys are mounted as files by jupyterhub_config.py */}}
        MINIO_ACCESS_KEY:
          valueFrom:
            secretKeyRef:
{{- if .Values.services.jupyterhub.singleUserNotebook.minio_user_dir_only }}
              key: user_dir_only_user_name
{{- else }}
              key: minio_access_key
{{- end }}
              name: minio-secret
        MINIO_PORT: '9001'
        MINIO_SECRET_KEY:
          valueFrom:
            secretKeyRef:
{{- if .Values.services.jupyterhub.singleUserNotebook.minio_user_dir_only }}
              key: user_dir_only_user_password
{{- else }}
              key: minio_secret_key
{{- end }}
              name: minio-secret
{{- end }}
        MINIO_URL: http://minio:9001
        SERVICE_TYPE: notebook
        STORAGE_API_URL: http://storage-api:31315
        STUDIO_API_URL: http://studio-api:8484
        TDM_API_URL: http://tdm-api:8686
        TELEGRAF_CADVISOR_URL: telegraf:8186
{{- if and .Values.traffic.istio.enabled ((.Values.traffic.istio).mtls).enabled }}
        IS_ISTIO_STRICT_MTLS_ENABLED: "True"
{{- end }}
      extraFiles:
        DefaultNotebook.ipynb:
          mountPath: /data/DefaultNotebook.ipynb
        ExploreNotebook.ipynb:
          mountPath: /data/ExploreNotebook.ipynb
        GenAIEvaluation.ipynb:
          mountPath: /data/GenAIEvaluation.ipynb
        README.md:
          mountPath: /data/README.md
      extraLabels:
        hub.jupyter.org/network-access-hub: 'true'
      extraNodeAffinity:
        preferred: []
        required: []
      extraPodAffinity:
        preferred: []
        required: []
      extraPodAntiAffinity:
        preferred: []
        required: []
      extraPodConfig: {}
      extraResource:
        guarantees: {}
{{- if and (and .Values.gpu.enabled .Values.gpu.gpu_config) .Values.services.jupyterhub.singleUserNotebook.gpu }}
        limits:
          nvidia.com/gpu: 1
{{- else }}
        limits: {}
{{- end }}
{{- if and (and .Values.gpu.enabled .Values.gpu.gpu_config) .Values.services.jupyterhub.singleUserNotebook.gpu }}
  {{- if .Values.gpu.gpu_config.tolerations }}
    {{- with .Values.gpu.gpu_config.tolerations }}
      extraTolerations:
        {{- toYaml . | nindent 8 }}
    {{- end }}
  {{- else }}
      extraTolerations: []
  {{- end }}
{{- end }}
      fsGid: 100
      image:
        {{- if .Values.image.imageNames.singleuserNotebook }}
        name: {{ .Values.image.imageNames.singleuserNotebook }}
        {{- else }}
        name: {{ template "snorkelflow.registryPrefix" . }}snorkelai/singleuser-notebook
        {{- end }}
        pullPolicy: Always
        pullSecrets:
        - {{ .Values.image.imagePullSecret | default "regcred" }}
        {{- if and (and .Values.gpu.enabled .Values.gpu.gpu_config .Values.services.jupyterhub.singleUserNotebook.gpu) (ne .Values.version "bazel") }}
        tag: {{ .Values.version }}-cuda
        {{- else }}
        tag: {{ .Values.version }}
        {{- end }}
      initContainers: []
      lifecycleHooks:
        postStart:
          exec:
            command:
            - /bin/sh
            - -c
            - python -m site --user-site | xargs mkdir -p
      memory:
        guarantee: {{ .Values.services.jupyterhub.singleUserNotebook.resources.memory_guarantee }}
        limit: {{ .Values.services.jupyterhub.singleUserNotebook.resources.memory_limit }}
      networkPolicy:
        allowedIngressPorts: []
        egress: []
        egressAllowRules:
          cloudMetadataServer: false
          dnsPortsCloudMetadataServer: true
          dnsPortsKubeSystemNamespace: true
          dnsPortsPrivateIPs: true
          nonPrivateIPs: true
          privateIPs: false
        enabled: true
        ingress: []
        interNamespaceAccessLabels: ignore
      networkTools:
        image:
          name: quay.io/jupyterhub/k8s-network-tools
          pullPolicy: null
          pullSecrets: []
          tag: 1.2.0
        resources: {}
{{- if and (and .Values.gpu.enabled .Values.gpu.gpu_config) .Values.services.jupyterhub.singleUserNotebook.gpu }}
  {{- if .Values.gpu.gpu_config.node_selectors }}
      nodeSelector:
        {{- range $key, $value := .Values.gpu.gpu_config.node_selectors }}
        {{ $key }}: "{{ $value }}"
        {{- end }}
  {{- else }}
      nodeSelector: {}
  {{- end }}
{{- end }}
      podNameTemplate: null
      profileList: []
      serviceAccountName: {{ .Values.services.jupyterhub.singleUserNotebook.serviceAccountName }}
      startTimeout: {{ .Values.services.jupyterhub.singleUserNotebook.startTimeout }}
      storage:
        capacity: 10Gi
        dynamic:
          pvcNameTemplate: claim-{username}{servername}
          storageAccessModes:
          - ReadWriteOnce
          storageClass: {{ .Values.services.jupyterhub.singleUserNotebook.storage.dynamicClass }}
          volumeNameTemplate: volume-{username}{servername}
        extraLabels: {}
        extraVolumeMounts: []
        extraVolumes: []
        homeMountPath: /data
        static:
          pvcName: null
          subPath: '{username}'
        type: {{ .Values.services.jupyterhub.singleUserNotebook.storage.type }}
      uid: 1000
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.

    Methods here can be imported by extraConfig in values.yaml
    """

    import os
    from collections.abc import Mapping
    from functools import lru_cache

    import yaml


    # memoize so we only load config once
    @lru_cache
    def _load_config():
        """Load the Helm chart configuration used to render the Helm templates of
        the chart from a mounted k8s Secret, and merge in values from an optionally
        mounted secret (hub.existingSecret)."""

        cfg = {}
        for source in ("secret/values.yaml", "existing-secret/values.yaml"):
            path = f"/usr/local/etc/jupyterhub/{source}"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg


    @lru_cache
    def _get_config_value(key):
        """Load value from the k8s ConfigMap given a key."""

        path = f"/usr/local/etc/jupyterhub/config/{key}"
        if os.path.exists(path):
            with open(path) as f:
                return f.read()
        else:
            raise Exception(f"{path} not found!")


    @lru_cache
    def get_secret_value(key, default="never-explicitly-set"):
        """Load value from the user managed k8s Secret or the default k8s Secret
        given a key."""

        for source in ("existing-secret", "secret"):
            path = f"/usr/local/etc/jupyterhub/{source}/{key}"
            if os.path.exists(path):
                with open(path) as f:
                    return f.read()
        if default != "never-explicitly-set":
            return default
        raise Exception(f"{key} not found in either k8s Secret!")


    def get_name(name):
        """Returns the fullname of a resource given its short name"""
        return _get_config_value(name)


    def get_name_env(name, suffix=""):
        """Returns the fullname of a resource given its short name along with a
        suffix, converted to uppercase with dashes replaced with underscores. This
        is useful to reference named services associated environment variables, such
        as PROXY_PUBLIC_SERVICE_PORT."""
        env_key = _get_config_value(name) + suffix
        env_key = env_key.upper().replace("-", "_")
        return os.environ[env_key]


    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.

        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged


    def get_config(key, default=None):
        """
        Find a config item of a given name & return it

        Parses everything as YAML, so lists and dicts are available too

        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split("."):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value


    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
kind: ConfigMap
metadata:
  name: jupyterhub-jupyterhubconfig-py-config-map
  namespace: {{ .Values.projectName }}
{{- end }}
