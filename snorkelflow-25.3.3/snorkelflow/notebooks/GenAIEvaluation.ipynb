{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1a701c-194b-48dd-b8e7-f1a619f8aba2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a08077-b913-47f9-ba6d-da621be588a6",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to set up an evaluation pipeline for GenAI-generated output, using Snorkel's Evaluation feature set.\n",
    "\n",
    "The evaluation workflow has four main phases:\n",
    "\n",
    "1. [Onboarding artifacts](#Phase-1:-Onboarding-artifacts)\n",
    "2. [Creating the initial evaluation benchmark](#Phase-2:-Creating-the-initial-evaluation-benchmark)\n",
    "3. [Refining the benchmark](#Phase-3:-Refining-the-benchmark)\n",
    "4. [Improving your GenAI application](#Phase-4:-Improving-your-GenAI-application)\n",
    "\n",
    "This notebook covers all four phases, using a small dataset of chatbot responses from an example GenAI chatbot application that answers questions about medical insurance for customers. For more about this example use case, read the [Evaluate GenAI output](https://docs.snorkel.ai/docs/0.95/user-guide/use-cases/genai-evaluation-notebook) tutorial in the Snorkel documentation.\n",
    "\n",
    "After running this notebook, you'll have a functional end-to-end benchmarking pipeline that allows you to track progress against multiple criteria for sequential generations of the chatbot response data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18453cc7-f43d-4ff6-942a-ec768c127096",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a29c41-7e67-4a62-86f8-3e028ca79db6",
   "metadata": {},
   "source": [
    "Have the following resources available before you run this notebook:\n",
    "\n",
    "- A Snorkel Flow instance\n",
    "- A Superadmin API key for Snorkel Flow\n",
    "- Amazon SageMaker with authentication secrets (used for fine tuning; you can skip that step)\n",
    "- The name of the LLM you want to use for any LLM-based functionality\n",
    "- The data files you will download below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d9c88-be74-4efb-919e-bd57b8ed8ac8",
   "metadata": {},
   "source": [
    "### Upload files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7c30f-7998-49a0-b1a1-b1a40eae5fce",
   "metadata": {},
   "source": [
    "Download these files from Snorkel. The four CSV files are the sample data, and the binary file is a model used to create a slice of Spanish-language data.\n",
    "\n",
    "Upload all of the files to Snorkel Flow, in the same directory as this notebook.\n",
    "\n",
    "- [eval-spanish.csv](https://snorkel-docs-downloads.s3.amazonaws.com/eval/eval-spanish.csv)\n",
    "- [eval-train-1.csv](https://snorkel-docs-downloads.s3.amazonaws.com/eval/eval-train-1.csv)\n",
    "- [eval-train-2.csv](https://snorkel-docs-downloads.s3.amazonaws.com/eval/eval-train-2.csv)\n",
    "- [eval-valid.csv](https://snorkel-docs-downloads.s3.amazonaws.com/eval/eval-valid.csv)\n",
    "- [eval-lid.176.bin](https://snorkel-docs-downloads.s3.amazonaws.com/eval/eval-lid.176.bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d700b-80d3-47cc-8915-94954cccb6e7",
   "metadata": {},
   "source": [
    "### User input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ba0f4-c1cf-421e-a5a7-77db4f6cce81",
   "metadata": {},
   "source": [
    "**<span style=\"color:red;\">This section requires input from the user.</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6595a-0d25-41a9-9dfd-b71757fe7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Snorkel workspace; the example uses \"default\"\n",
    "workspace_uid = 1\n",
    "workspace_name = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2a461-c363-42e7-aef6-c65d256c3b94",
   "metadata": {},
   "source": [
    "This notebook creates many assets like datasets, apps, and slices. If you run asset creation cells multiple times, you will receive naming conflict errors because the asset was created on the first run. If you want to re-run this notebook, replace the `app_name` below with a new name and start from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429aa78-c14c-4988-bc34-4c9913f37b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your Superadmin Snorkel Flow API key\n",
    "api_key = None  # replace this with your SnorkelFlow API key\n",
    "\n",
    "# Define your app name\n",
    "app_name = \"evaluation-example-app\"\n",
    "\n",
    "# Pick which LLM you want to use for LLM-based functionality\n",
    "model_name = \"openai/gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa74777-fefa-4e7a-bd0b-1ce0ec9c78e5",
   "metadata": {},
   "source": [
    "Add your Amazon SageMaker access details. This is needed for the demonstration of fine-tuning the model powering your GenAI application. If you want to skip the fine-tuning portion, the rest of the notebook will still function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5337b29-67c5-45fc-b322-05ab8e9b6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID_SECRET = \"YOUR_ID_SECRET\"\n",
    "AWS_SECRET_ACCESS_KEY_SECRET = \"YOUR_KEY_SECRET\"\n",
    "SAGEMAKER_EXECUTION_ROLE_SECRET = \"YOUR_ROLE_SECRET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed685d20-d1fa-43cd-99a9-9b4949dea17f",
   "metadata": {},
   "source": [
    "Once you've completed the user input section, you can run the rest of the notebook as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc97d8a-dd68-4df1-99e8-63a3894cd656",
   "metadata": {},
   "source": [
    "### Set up Snorkel Flow app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c02834-6a23-401c-85ae-4a6c4684ee60",
   "metadata": {},
   "source": [
    "Import the Snorkel SDK and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bcb7f-1274-40ae-bd47-ecbf99c2d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR\n",
    ")  # Set the root logger to only show error-level messages\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\"\n",
    ")  # use warnings.resetwarnings() if you want to display warnings for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270299f-4fd6-4185-9af3-6d6660602147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import snorkelflow.client as sf\n",
    "import snorkelflow.sdk as sf_sdk\n",
    "from snorkelflow.sdk.fine_tuning_app import FineTuningApp\n",
    "from snorkelflow.types.finetuning import (\n",
    "    AnnotationStrategy,\n",
    "    FineTuningAppConfig,\n",
    "    FineTuningColumnType,\n",
    ")\n",
    "from snorkelflow.types.source import ModelSourceMetadata\n",
    "from snorkelflow.sdk import Dataset\n",
    "from snorkelflow.sdk import ModelNode\n",
    "from snorkelflow.sdk.slices import Slice\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d3f6e-2af6-4210-a7da-62329859beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Snorkel Flow\n",
    "ctx = sf.SnorkelFlowContext.from_kwargs(\n",
    "    api_key=api_key,\n",
    "    workspace_name=workspace_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e3b11-b26f-4d2d-96cd-cee2ca657fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the first train and valid data splits\n",
    "df_t1 = pd.read_csv(\"./eval-train-1.csv\")\n",
    "df_v1 = pd.read_csv(\"./eval-valid.csv\")\n",
    "print(df_t1.columns)\n",
    "print(df_v1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca2009-da6d-4455-81d9-32cc6174fcbd",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Index(['question', 'response', 'retrieved_context_clean', 'prompt_prefix'], dtype='object')\n",
    "Index(['question', 'response', 'retrieved_context_clean', 'prompt_prefix'], dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973de14a-8fbb-4d56-a85e-78adf0c77f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning app config, map data fields, and create the new Snorkel Flow app\n",
    "cm = {\n",
    "    \"question\": \"instruction\",\n",
    "    \"response\": \"response\",\n",
    "    \"prompt_prefix\": \"prompt_prefix\",\n",
    "    \"retrieved_context_clean\": \"context\",\n",
    "}\n",
    "app_config = FineTuningAppConfig(column_mappings=cm)\n",
    "\n",
    "FineTuningApp.create(app_name, app_config)\n",
    "ft_app = FineTuningApp.get(app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d182f08-a723-49be-9b9a-ae818c2e8c5d",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully created dataset evaluation-example-app with UID XXXX.\n",
    "Successfully created label schema Response acceptance with UID XXXX.\n",
    "Successfully created dataset view Single response view with uid XXXX\n",
    "Successfully created fine tuning application evaluation-example-app with uid XXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece5540-8791-4f39-9b5d-a05da9f6018e",
   "metadata": {},
   "source": [
    "This workflow introduces a model_source object, which maps data sources in the parent dataset to different experiments. For example, our initial data is coming in from our alpha experiment, and it has a different model source id than data we will onboard later from a beta experiment. This object distinguishes metrics across evaluations and determine the data a user is developing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b90b3e-4f13-4944-8114-1b795bcfcab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the base LLM provider\n",
    "llama3_8B_v0 = \"llama3-8B-v0\"\n",
    "experiment1 = ft_app.register_model_source(\n",
    "    llama3_8B_v0, metadata=ModelSourceMetadata(model_name=llama3_8B_v0)\n",
    ")[\"source_uid\"]\n",
    "\n",
    "# Import data to the app\n",
    "ft_app.import_data(data=df_t1, split=\"train\", source_uid=experiment1, sync=True)\n",
    "ft_app.import_data(data=df_v1, split=\"valid\", source_uid=experiment1, sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a6313-4451-4259-995d-657c0e574f9b",
   "metadata": {},
   "source": [
    "The data import can take 2-5 minutes to complete.\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "+0.59s Waiting for next available worker (position in queue: 1)\n",
    "\n",
    "...\n",
    "\n",
    "+68.02s (100%) Transactions committed.\n",
    "\n",
    "'rq-sbU06y4a_engine-YUYK_prep-and-ingest-fine-tuning-data'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf3124-ddf1-4cc7-9743-2f043339443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this cell if you want to import an augmented dataset\n",
    "# augmented_df = sf.augment_dataset(\n",
    "#     dataset=ft_app.dataset_uid,\n",
    "#     x_uids=ft_app.get_dataframe(\"train\").index.tolist()[:5],\n",
    "#     model_name=model_name,\n",
    "#     runs_per_prompt=2,\n",
    "#     fields=[\"question\", \"response\"],\n",
    "#     temperature=1.5,\n",
    "# )\n",
    "#\n",
    "# ft_app.import_data(\n",
    "#     data=augmented_df,\n",
    "#     split=\"train\",\n",
    "#     source_uid=ft_app.register_model_source(\"input_data_augmentation\", metadata=ModelSourceMetadata(model_name=\"gpt4o\"))['source_uid'],\n",
    "#     sync=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0f972-490f-4025-982b-72ae3dc423bf",
   "metadata": {},
   "source": [
    "# Phase 1: Onboarding artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3446b59-efc0-4f3d-a885-d2682c700a9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define evaluation criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44b2e8-1a4c-4034-a053-41d32d42eed5",
   "metadata": {},
   "source": [
    "Before you can evaluate the chatbot's response quality, you need to define what constitutes a high quality response.\n",
    "\n",
    "In this example, you define six individual criteria for a high-quality response:\n",
    "\n",
    "- Completeness\n",
    "- Correctness\n",
    "- Polite Tone\n",
    "- Retrieved Context Accuracy\n",
    "- unit_test_contains_pii\n",
    "- unit_test_mentions_competitors\n",
    "\n",
    "The chatbot responses can be assessed against these criteria in multiple ways:\n",
    "\n",
    "- Collect ground truth ratings from domain experts. For example, your support team could label responses as having a \"Polite Tone\" or not.\n",
    "- Use Snorkel evaluators to assess responses programattically. When you use evaluators, you can scale quality measurements and evaluate subsequent experiments with little involvement from our domain experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257db6d1-f4a6-4dfa-8dea-4036261a232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a name and description for each criterion\n",
    "criteria = [\n",
    "    {\n",
    "        \"name\": \"Completeness\",\n",
    "        \"description\": \"Completeness refers to the organization and thoroughness of the chatbot's response. It evaluates how well the chatbot breaks down complex concepts, provides logical explanations, and uses appropriate examples or analogies to aid understanding. Evaluators should assess whether the explanation covers all necessary information, details, and components needed to provide a thorough and satisfactory answer.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Correctness\",\n",
    "        \"description\": \"Correctness measures the accuracy and relevance of the chatbot's response to the user's query. It involves assessing whether the provided information is factually correct, up-to-date, and directly addresses the user's question or request. Evaluators should consider the overall accuracy of the response, including any claims, statistics, or data mentioned, and determine if the information is partially or fully correct in relation to the query.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Polite Tone\",\n",
    "        \"description\": \"Polite Tone assesses the chatbot's ability to maintain a respectful and courteous demeanor in its responses. Evaluators should consider the use of polite language, appropriate greetings and sign-offs, and the overall friendliness of the interaction. They should also note whether the chatbot responds to frustration or criticism with patience and empathy, avoiding dismissive or confrontational language, and maintaining professionalism throughout the conversation.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Retrieved Context Accuracy\",\n",
    "        \"description\": \"This criterion evaluates how well the AI's response aligns with and correctly uses retrieved information. Consider relevance to the instruction, factual correctness, proper context, completeness, consistency with retrieved content, source attribution (if applicable), appropriate synthesis with general knowledge, and handling of uncertainty. Assess whether the response directly addresses the query using pertinent retrieved data, avoids misrepresentation or omission of important details, and logically combines retrieved information with the AI's knowledge base. If information is incomplete or ambiguous, check if the AI appropriately expresses limitations. Rate the pair on a scale based on how accurately the response utilizes retrieved information to address the given instruction.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"unit_test_contains_pii\",\n",
    "        \"description\": \"This criterion assesses whether the chatbot's response includes any information that could be used to identify a specific individual. Evaluators should look for explicit mentions of personal details such as names, addresses, phone numbers, or email addresses. They should also be aware of indirect references or contextual information that, when combined, could lead to identification of an individual.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"unit_test_mentions_competitors\",\n",
    "        \"description\": \"This criterion evaluates whether the chatbot's response mentions or discusses competing products, services, or companies, especially when not directly relevant to the user's query. Evaluators should identify any mentions of competitors, consider their relevance to the user's question, and flag instances where the chatbot compares itself to competitors or makes inappropriate recommendations about them.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162e8b3-1ec6-4712-b181-4f753b992693",
   "metadata": {},
   "source": [
    "Evaluation criteria are defined as dataset label schemas in Snorkel Flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a02c15-1f39-49c1-beb3-0212e12ce8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset label schema for each evaluation criterion\n",
    "d = sf_sdk.Dataset.get(app_name)\n",
    "for schema in criteria:\n",
    "    try:\n",
    "        d.create_label_schema(\n",
    "            name=schema[\"name\"],\n",
    "            data_type=\"text\",\n",
    "            task_type=\"classification\",\n",
    "            description=schema[\"description\"],\n",
    "            label_map=[\"ACCEPT\", \"REJECT\"],\n",
    "        )\n",
    "    except:\n",
    "        print(schema)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf48c9-c28a-4d60-8bab-4f1a6efed120",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "Successfully created label schema Completeness with UID XXXX.\n",
    "Successfully created label schema Correctness with UID XXXX.\n",
    "Successfully created label schema Polite Tone with UID XXXX.\n",
    "Successfully created label schema Retrieved Context Accuracy with UID XXXX.\n",
    "Successfully created label schema unit_test_contains_pii with UID XXXX.\n",
    "Successfully created label schema unit_test_mentions_competitors with UID XXXX.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e618e-b6a9-49ab-8c02-c89606818bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset view for the label schemas\n",
    "# This allows you to create annotation batches targeted to your criteria\n",
    "from snorkelflow.client.dataset_views import create_dataset_view\n",
    "\n",
    "cm_flipped = {v: k for k, v in cm.items()}\n",
    "label_schema_uids = [label_schema.uid for label_schema in d.label_schemas]\n",
    "sf.create_dataset_view(\n",
    "    dataset=app_name,\n",
    "    name=\"Native GenAI Viewer\",\n",
    "    view_type=\"single_llm_response_view\",\n",
    "    column_mapping=cm_flipped,\n",
    "    label_schema_uids=label_schema_uids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4eac9-66f9-49ef-9882-245f43d7c212",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "{'dataset_uid': XXXX,\n",
    " 'name': 'Native GenAI Viewer',\n",
    " 'view_type': 'single_llm_response_view',\n",
    " 'column_mapping': {'instruction': 'question',\n",
    "  'response': 'response',\n",
    "  'prompt_prefix': 'prompt_prefix',\n",
    "  'context': 'retrieved_context_clean'},\n",
    " 'label_schema_uids': [YOUR_SCHEMA_UIDS],\n",
    " 'dataset_view_uid': XXXX}\n",
    "```\n",
    "\n",
    "The dataset view created above allows annotators to mark either `ACCEPT` or `REJECT` for the evaluation criteria.\n",
    "\n",
    "To capture additional valuable feedback from annotators, you will want to create a free text feedback field too. Annotators can use this field to write gold standard responses, or to give a rationale for why they rejected a particular response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda3824-d507-4c30-9719-190f334d486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a name and description for the free text field for annotators\n",
    "free_text_criteria = [\n",
    "    {\n",
    "        \"name\": \"rationale_if_incorrect\",\n",
    "        \"description\": \"Document the reasoning behind marking a response as incorrect and provide valuable feedback for improving the LLMs performance.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the label schema for the rationale field\n",
    "for schema in free_text_criteria:\n",
    "    ctx.tdm_client.post(\n",
    "        \"label-schemas\",\n",
    "        json=dict(\n",
    "            {\n",
    "                \"dataset_uid\": ft_app.dataset_uid,\n",
    "                \"name\": schema[\"name\"],\n",
    "                \"description\": schema[\"description\"],\n",
    "                \"data_type\": \"text\",\n",
    "                \"task_type\": \"classification\",\n",
    "                \"is_multi_label\": False,\n",
    "                \"label_map\": {},\n",
    "                \"label_descriptions\": {},\n",
    "                \"primary_text_field\": \"text\",\n",
    "                \"is_text_label\": True,\n",
    "            }\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c7f52-8a0a-4b06-a895-4403b513d438",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create annotation batch for the first set of chatbot responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ac575-1077-4c2c-87c6-e75d4a30b437",
   "metadata": {},
   "source": [
    "Get the first generation of chatbot responses ready for evaluation.\n",
    "\n",
    "Before creating the batch, create a reusable function that creates targeted annotation batches from different generations of your chatbot response datasets. These generations are called experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df590b88-f4df-4698-8bf5-2979de623bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to get the UID for an experiment\n",
    "def get_experiment_uid(experiment_name):\n",
    "    # Check if the 'source' key exists and has the correct structure\n",
    "    res = None\n",
    "    for (\n",
    "        k,\n",
    "        v,\n",
    "    ) in ft_app.datasource_metadata.items():\n",
    "        if v[\"source\"][\"source_name\"] == experiment_name:\n",
    "            # Return the model name\n",
    "            res = v[\"source_uid\"]\n",
    "    return res\n",
    "\n",
    "\n",
    "# Define a helper function to create a new annotation batch with the dataset from a particular experiment\n",
    "def create_custom_batch(\n",
    "    name,\n",
    "    dataset_name,\n",
    "    experiment_name,\n",
    "    split,\n",
    "    label_schema_names,\n",
    "    batch_size=None,\n",
    "    x_uids=[],\n",
    "):\n",
    "    from snorkelflow.sdk import Dataset\n",
    "\n",
    "    ft_app = FineTuningApp.get(dataset_name)\n",
    "    d = Dataset.get(dataset=dataset_name)\n",
    "    schemas_in_batch = []\n",
    "    for ls in d.label_schemas:\n",
    "        if ls.name in label_schema_names:\n",
    "            schemas_in_batch.append(ls)\n",
    "    experiment_uid = get_experiment_uid(experiment_name=experiment_name)\n",
    "    if experiment_uid:\n",
    "        res = d.create_batches(\n",
    "            name=name,\n",
    "            label_schemas=schemas_in_batch,\n",
    "            batch_size=batch_size,\n",
    "            x_uids=x_uids\n",
    "            + list(\n",
    "                ft_app.get_dataframe(split=split, source_uids=[experiment_uid]).index\n",
    "            ),\n",
    "        )\n",
    "        return res\n",
    "    else:\n",
    "        return \"No experiment with that name found, batch not created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349a21-9c86-458e-ac80-f71a1bb92eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the annotation batch for the first set of responses, from the \"train\" split imported earlier\n",
    "create_custom_batch(\n",
    "    name=\"custom-sdk-batch\",\n",
    "    dataset_name=app_name,\n",
    "    split=\"train\",\n",
    "    experiment_name=llama3_8B_v0,\n",
    "    label_schema_names=[\"Completeness\", \"Correctness\"],\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082cb1d-0172-472b-9d8e-9085d607258e",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "Successfully created 1 batches.\n",
    "\n",
    "[<snorkelflow.sdk.batch.Batch at xxxxxxxxxxxxxx>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced90fe0-241f-4f3d-88ec-93ea6c43f07f",
   "metadata": {},
   "source": [
    "## Define evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274d408-41d7-405f-b3a5-a99cbea20ed7",
   "metadata": {},
   "source": [
    "Evaluators are functions that programmatically check whether an LLM response satisfies a criterion. They greatly accelerate evaluation, reducing or eliminating the need for manual annotation of each response against each criterion.\n",
    "\n",
    "An evaluator can be anything that has the signature:\n",
    "\n",
    "```\n",
    "(prompt, response, [context]) → {0,1}\n",
    "```\n",
    "\n",
    "Examples include:\n",
    "\n",
    "- An off-the-shelf classifier (e.g. for PII/toxicity)\n",
    "- A prompted LLM, the key ingredient of an LLM-as-a-judge evaluator\n",
    "- A heuristic rule\n",
    "- An automated or heuristic comparison to an SME-annotated gold dataset (for example, a similarity score, calculated from an embedding or by an LLM, between the responses being evaluated and gold SME responses)\n",
    "- A predictive model built with programmatic supervision\n",
    "\n",
    "This notebook contains heuristic and LLM-as-judge evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f703aaa-c667-4797-83bf-ef7b1cf8f6e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluator: Heuristic evaluator checks for mention of competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc00c3-76b9-4b43-b348-56cfd2280010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a heuristic evaluator\n",
    "# This evaluator assesses whether the chatbot's response mentions competitors\n",
    "def mentions_competitors(df: pd.DataFrame) -> float:\n",
    "    import pandas as pd\n",
    "\n",
    "    competitor_list = [\n",
    "        \"Anthem\",\n",
    "        \"UnitedHealth Group\",\n",
    "        \"Cigna\",\n",
    "        \"Aetna (CVS Health)\",\n",
    "        \"Humana\",\n",
    "        \"Centene Corporation\",\n",
    "        \"Kaiser Permanente\",\n",
    "        \"Blue Cross Blue Shield Association\",\n",
    "        \"Molina Healthcare\",\n",
    "        \"Health Care Service Corporation (HCSC)\",\n",
    "        \"Highmark\",\n",
    "    ]\n",
    "    # Evaluators are calculated on the fly slice-wise. Account for slices that don't have an x_uid for a given split.\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f\"No samples found\")\n",
    "    df[\"response_has_competitors\"] = df[\"response\"].apply(\n",
    "        lambda x: any(s in x for s in competitor_list)\n",
    "    )\n",
    "    return df[\"response_has_competitors\"].mean()\n",
    "\n",
    "\n",
    "# Test the function on a sample of data before registering it as a custom metric\n",
    "t = ft_app.get_dataframe(split=\"train\")\n",
    "t_comp = mentions_competitors(t)\n",
    "print(f\"Percentage of sample data that mentions competitors: {t_comp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf2001-b464-4f64-88e0-26c774b56b17",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Percentage of sample data that mentions competitors: 0.06\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f4ca5-008d-4593-a299-4531cae76b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the metric with the application\n",
    "ft_app.register_custom_metric(\n",
    "    metric_name=\"Heuristic | Mentions Competitors\",\n",
    "    metric_func=mentions_competitors,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ddbf1-52ef-43ad-91d0-13aad4ec4040",
   "metadata": {},
   "source": [
    "#### Evaluator: LLM-as-judge evaluator checks for response completeness\n",
    "\n",
    "Use an LLM-as-judge to measure the Completeness criterion. Start by listing available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a1beb-1bd1-4c91-9813-05c8256df1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of available foundation models to prompt\n",
    "sf.get_external_model_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d332d37-bd06-4cf7-a946-830f26a1c4cb",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "{'openai/gpt-4o': 'https://api.openai.com/v1/chat/completions',\n",
    " 'deepset/roberta-large-squad2': 'https://pxyrwkggvs7nr8x7.us-east-1.aws.endpoints.huggingface.cloud',\n",
    " 'google/flan-t5-xl': 'https://edpowgnae37imiew.us-east-1.aws.endpoints.huggingface.cloud',\n",
    " 'vertexai_lm/text-bison@001': 'https://cloud.google.com/vertex-ai',\n",
    " 'openai/gpt-4o-mini': 'https://api.openai.com/v1/chat/completions',\n",
    " 'vertexai_lm/chat-bison@001': 'https://cloud.google.com/vertex-ai',\n",
    " 'vertexai_lm/gemini-1.0-pro': 'https://cloud.google.com/vertex-ai',\n",
    " 'impira/layoutlm-document-qa': 'https://uep8go5hobns4u1w.us-east-1.aws.endpoints.huggingface.cloud',\n",
    " 'openai/gpt-3.5-turbo': 'https://api.openai.com/v1/chat/completions',\n",
    " 'openai/gpt-4': 'https://api.openai.com/v1/chat/completions',\n",
    " 'mistral.mistral-7b-instruct-v0:2': 'http://Bedroc-Proxy-6ZRulff8IuuZ-1976839026.us-west-2.elb.amazonaws.com/api/v1',\n",
    " 'meta.llama3-8b-instruct-v1:0': 'http://Bedroc-Proxy-6ZRulff8IuuZ-1976839026.us-west-2.elb.amazonaws.com/api/v1',\n",
    " 'azure_openai/jioh-gpt-4o-mini': 'https://jiohopenaiinstance.openai.azure.com/chat/completions'}\n",
    "```\n",
    "\n",
    "Engineer a prompt that enables an LLM to act as a judge in assessing whether the health insurance chatbot's answers pass or fail the Completeness criterion.\n",
    "\n",
    "After defining the prompt, it's a best practice to run it on a subset of the data points before running it over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18219cb5-823f-42c9-b678-313b7b60252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an LLMAJ evaluator with a custom prompt and model\n",
    "from snorkelflow.evaluation.metric_schema import CustomPromptMetricSchema\n",
    "\n",
    "pt = \"\"\"As an AI judge evaluating a healthcare copilot, your task is to assess the Completeness of the chatbot's responses. Focus on how well the chatbot clarifies complex medical concepts, provides logical explanations, and uses relevant examples or analogies. Consider the following:\n",
    "\n",
    "1. Ease of comprehension: Is the explanation easy to understand for a general audience?\n",
    "2. Logical flow: Does the response progress in a coherent, step-by-step manner?\n",
    "3. Use of examples/analogies: Are appropriate comparisons made to aid comprehension?\n",
    "4. Jargon management: Is medical terminology adequately explained when used?\n",
    "5. Completeness: Does the explanation cover all necessary aspects of the topic?\n",
    "\n",
    "If the response is very complete according to the items outlined above, give it a 1.\n",
    "If the response is not complete according to the items outlined above, give it a 0.\n",
    "\n",
    "Only respond with one number, either 0 or 1, and NO OTHER NUMBERS OR TEXT.\n",
    "    Response: {response}\n",
    "\"\"\"\n",
    "\n",
    "LLMAJ_completeness = CustomPromptMetricSchema(\n",
    "    metric_type=\"custom_prompt\",\n",
    "    display_name=\"LLMAJ | Completeness\",\n",
    "    description=\"LLMAJ for Completeness Criteria\",\n",
    "    display_style=\"percentage\",\n",
    "    prompt_text=pt,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e85c6-a859-4e53-9ae5-5cce0d3b507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkelflow.client_v3.evaluation import preview_custom_prompt_metric\n",
    "\n",
    "# Test the LLMAJ prompt for a sample of the chatbot responses\n",
    "a = list(ft_app.get_dataframe().index)[:5]\n",
    "res = preview_custom_prompt_metric(\n",
    "    dataset=ft_app.dataset_uid,\n",
    "    x_uids=a,\n",
    "    metric_schema=LLMAJ_completeness,\n",
    ")\n",
    "res[[\"question\", \"response\", \"perplexity\", \"score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3b345-a028-4e6d-892d-74a654767463",
   "metadata": {},
   "source": [
    "It looks like our LLM-as-judge is generating the values that we'd expect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c0eca-9fa6-497c-b3ac-1c98f4cd4210",
   "metadata": {},
   "source": [
    "#### Register the LLMAJ evaluator\n",
    "\n",
    "Register the LLMAJ completeness evaluator in Snorkel Flow so its results are available to the application's evaluation dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f406b7-1e40-442d-a935-a7bcff9e9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the metric with the application\n",
    "ft_app.register_metric(LLMAJ_completeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ea033-5d4f-4451-8d76-a132c4b340ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define reference prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cd4a5-7fe3-41f5-9b41-b3b7bd25319a",
   "metadata": {},
   "source": [
    "Define the set of prompts that you want to use across different generations of evaluation.\n",
    "\n",
    "As you fine tune the model powering your chatbot, the responses will change, but these prompts will stay the same so you have a consistent reference point for the evaluation scores. You may add to this set of prompts if you want to evaluate your model on a wider array of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392bf50-aebb-454c-bc7c-9430ca90f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkelflow.sdk import ModelNode\n",
    "\n",
    "node = ModelNode.get(node_uid=ft_app.model_node_uid)\n",
    "df = node.get_dataframe()\n",
    "df[\"question\"].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d56035-4cb4-4d03-b4e3-11f647982ae6",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "array(['What is the procedure for updating personal information and dependents for coverage?',\n",
    "       'What are the specific age limits for dependent coverage under my plan?',\n",
    "       \"How to challenge partial denial of a dependent's medical claim?\",\n",
    "       'Can you explain the process of prior authorization for specific treatments or medications?',\n",
    "       'How is coverage determined for experimental or investigational treatments?'],\n",
    "      dtype=object)\n",
    "```\n",
    "\n",
    "Next, let's retrieve some entries from the sample dataset so we can examine the data in more detail. The dataset has several fields:\n",
    "\n",
    "- **question**: The instruction passed from the user to the alpha version of the GenAI chatbot application.\n",
    "- **response**: The response from the healthcare chatbot.\n",
    "- **preference**: Some pre-collected ground truth measuring overall response quality.\n",
    "- **metadata_label**: We trained an intent classifier offline and are using its values here during eval to help define data slices.\n",
    "- **retrieved_context_clean**: The chatbot uses a retrieval augmented generation (RAG) pipeline to aid in factually grounding responses in the organization's knowledge base. To render this field properly for annotators, ensure that each cell in this column contains valid JSON.\n",
    "- **prompt_prefix**: This is the static portion of the prompt that's passed to the LLM with each user interaction. This is also referred to as the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3169f13-ba2d-4c5b-b526-7c7ac9edeb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a few data points\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0228f2-4de5-4d71-9bd2-a87c7bfec923",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Jupyter should render a table with some entries from the sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df924af8-65c1-4e60-8c0e-be2cdc1611e2",
   "metadata": {},
   "source": [
    "## Define data slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ef43f-fdcc-4aa4-b16a-5e0dd67f6e26",
   "metadata": {},
   "source": [
    "Data slices sort and codify inputs with different characteristics. These should relate to categories of users or categories of questions that matter to your business.\n",
    "\n",
    "In this healthcare example, you will define slices for:\n",
    "\n",
    "- Verbose questions\n",
    "- Questions written in Spanish\n",
    "- Disputes\n",
    "- Questions about out-of-network coverage\n",
    "- Questions about premiums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2564d43-0bfa-45c0-a20c-71a4081988a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from snorkelflow.sdk.slices import Slice, SliceConfig\n",
    "from snorkelflow.sdk import ModelNode\n",
    "from templates import RegexTemplateSchema, KeywordTemplateSchema\n",
    "from snorkelflow.utils.graph import DEFAULT_GRAPH\n",
    "\n",
    "node = ModelNode.get(node_uid=ft_app.model_node_uid)\n",
    "\n",
    "\n",
    "# Write a slicing function\n",
    "def apply_slice(slicing_fn):\n",
    "    df = node.get_dataframe()\n",
    "    slice_mask = df.progress_apply(slicing_fn, axis=1)\n",
    "    df_sliced = df[slice_mask]\n",
    "    x_uids = list(df_sliced.index)\n",
    "    slice_name = slicing_fn.__name__\n",
    "    slice_percent = len(df_sliced) / len(df)\n",
    "    print(\n",
    "        f\"Applied '{slice_name}' slice (n={len(df_sliced)}, {slice_percent*100:.2f}%)\"\n",
    "    )\n",
    "    return df_sliced, x_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea23096-61db-4947-8d55-43b6334b1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slice of data for verbose questions\n",
    "def verbose_question(x):\n",
    "    return len(x.question) > 100\n",
    "\n",
    "\n",
    "verbose_df, verbose_x_uids = apply_slice(verbose_question)\n",
    "verbose_slice = Slice.create(name=\"verbose_question\", dataset=ft_app.dataset_uid)\n",
    "verbose_slice.add_x_uids(verbose_x_uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac413b-68fb-4410-88a3-6f99b2b4b72e",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Applied 'verbose_question' slice (n=9, 4.50%)\n",
    "Successfully created slice verbose_question with UID XXXX.\n",
    "Successfully added 9 datapoints to slice verbose_question.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10420d80-2dc4-4d07-a61c-b01b063a9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slice of data for questions in Spanish\n",
    "!pip install fasttext-wheel\n",
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(\"./eval-lid.176.bin\")\n",
    "\n",
    "\n",
    "def is_spanish(x):\n",
    "    predictions = model.predict(x.question)\n",
    "    lang = predictions[0][0].split(\"__label__\")[-1]\n",
    "    return lang == \"es\"\n",
    "\n",
    "\n",
    "spanish_df, spanish_x_uids = apply_slice(is_spanish)\n",
    "spanish_slice = Slice.create(name=\"spanish\", dataset=ft_app.dataset_uid)\n",
    "spanish_slice.add_x_uids(spanish_x_uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0802556-c097-48a7-be2e-478cda2ab991",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Defaulting to user installation because normal site-packages is not writeable\n",
    "Collecting fasttext-wheel\n",
    "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
    "\n",
    "...\n",
    "\n",
    "Applied 'is_spanish' slice (n=0, 0.00%)\n",
    "Successfully created slice spanish with UID XXXX.\n",
    "Successfully added 0 datapoints to slice spanish.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b0a70-4cc1-4f1b-8b0b-4ba553225833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slice of data for disputes\n",
    "topic_disputes_slice = Slice.create(\n",
    "    name=\"topic_disputes\",\n",
    "    dataset=ft_app.dataset_uid,\n",
    "    config=SliceConfig(\n",
    "        templates=[\n",
    "            RegexTemplateSchema(\n",
    "                field=\"question\",\n",
    "                regex_pattern=r\"\\b(appeal|appealed|dispute|disputed|disputes)\\b\",\n",
    "                case_sensitive=False,\n",
    "            )\n",
    "        ],\n",
    "        graph=DEFAULT_GRAPH,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a slice of data for questions about out of network topics\n",
    "topic_out_of_network_slice = Slice.create(\n",
    "    name=\"topic_out_of_network\",\n",
    "    dataset=ft_app.dataset_uid,\n",
    "    config=SliceConfig(\n",
    "        templates=[\n",
    "            RegexTemplateSchema(\n",
    "                field=\"question\",\n",
    "                regex_pattern=r\"\\b(network)\\b\",\n",
    "                case_sensitive=False,\n",
    "            )\n",
    "        ],\n",
    "        graph=DEFAULT_GRAPH,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a slice of data for questions about premiums\n",
    "topic_premiums_slice = Slice.create(\n",
    "    name=\"topic_premiums\",\n",
    "    dataset=ft_app.dataset_uid,\n",
    "    config=SliceConfig(\n",
    "        templates=[\n",
    "            RegexTemplateSchema(\n",
    "                field=\"question\",\n",
    "                regex_pattern=r\"\\b(PREMIUMS)\\b\",\n",
    "                case_sensitive=False,\n",
    "            )\n",
    "        ],\n",
    "        graph=DEFAULT_GRAPH,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2020f-dc31-4b66-aa2f-bea2c5f86a33",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully created slice topic_disputes with UID XXXX.\n",
    "Successfully created slice topic_out_of_network with UID XXXX.\n",
    "Successfully created slice topic_premiums with UID XXXX.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04742a4b-3c99-40db-97e9-b60e2e39c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to reapply slices\n",
    "def reapply_slices(dataset_uid):\n",
    "    # Manual\n",
    "    verbose_df, verbose_x_uids = apply_slice(verbose_question)\n",
    "    verbose_slice = Slice.get(dataset=dataset_uid, slice=\"verbose_question\")\n",
    "    verbose_slice.add_x_uids(verbose_x_uids)\n",
    "\n",
    "    spanish_df, spanish_x_uids = apply_slice(is_spanish)\n",
    "    spanish_slice = Slice.get(dataset=dataset_uid, slice=\"spanish\")\n",
    "    spanish_slice.add_x_uids(spanish_x_uids)\n",
    "\n",
    "    # Programmatic\n",
    "    slices = Slice.list(dataset=dataset_uid)\n",
    "    slice_uids = [sl.slice_uid for sl in slices]\n",
    "    slice_uids\n",
    "\n",
    "    # Programmatic\n",
    "    ctx.tdm_client.post(\n",
    "        f\"/dataset/{dataset_uid}/apply\",\n",
    "        json=dict({\"dataset_uid\": dataset_uid, \"slice_uids\": slice_uids}),\n",
    "    )\n",
    "    print(f\"Successfully reapplied slices to the dataset {dataset_uid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413e4de-16f2-4401-8070-3502616ce9df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Phase 2: Creating the initial evaluation benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697fff4f-5ebc-4821-99ce-69a71d5664ac",
   "metadata": {},
   "source": [
    "After onboarding all the artifacts, it's time to run the initial benchmark and view the results in the evaluation dashboard in Snorkel Flow.\n",
    "\n",
    "There are two ways to create evaluation reports: 1) from the fine-tuning application, and 2) as part of the standalone `evaluation` module. For simple evaluation reports, Snorkel recommends using Method 1, but for more complex, multi-criteria, multi-evaluator reports, Snorkel recommends Method 2.\n",
    "\n",
    "Note: Any evaluation report that includes an LLM-based metric will take longer to compute because of the API calls to the model. The first run will take longer and subsequent runs will be quicker thanks to our built-in caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1c530-7d4e-4a4d-b074-67e27ed76f73",
   "metadata": {},
   "source": [
    "## Create a simple evaluation report\n",
    "\n",
    "Use this for simpler evaluation reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83118dc9-291b-4b7e-979e-d0af5fd99e25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a simple, initial evaluation report\n",
    "ft_app.create_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f78815-a14b-4e1d-b0d6-3d1ce98cfe0a",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "No quality models found, skipping model acceptance rate metric\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "+0.58s Slice apply job for dataset XXXX\n",
    "+0.59s Starting evaluation module compute metrics job\n",
    "+1.28s Found and loaded datasources for models llama3-8B-v0 in dataset XXXX train split.\n",
    "+1.97s No valid data found to calculate the metric Heuristic | Mentions Competitors.\n",
    "\n",
    "...\n",
    "\n",
    "{'dataset_uid': XXXX,\n",
    "\n",
    "...\n",
    "\n",
    "     'global': 100.0}}},\n",
    "  'test': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2748a65-9698-4afd-8fcf-432f8c3453cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Define evaluation report helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692b1ba-4161-4775-af4e-cbd33e0905f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to get the latest report UID\n",
    "def get_latest_report_uid(dataset_uid):\n",
    "    reports = ctx.tdm_client.get(\n",
    "        f\"/dataset/{dataset_uid}/evaluation-report\",\n",
    "    )\n",
    "    return reports[0].get(\"evaluation_report_uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf531e5-008f-4389-bf86-e15ff696cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to update the report description\n",
    "def update_report_description(dataset_uid, eval_report_uid, description):\n",
    "    ctx.tdm_client.put(\n",
    "        f\"/dataset/{dataset_uid}/evaluation-report/{eval_report_uid}\",\n",
    "        json=dict(\n",
    "            {\n",
    "                \"dataset_uid\": dataset_uid,\n",
    "                \"evaluation_report_uid\": eval_report_uid,\n",
    "                \"additional_notes\": description,\n",
    "            }\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212abdd9-4b74-4ef5-8bd7-aac87c78a918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create an evaluation report using multiple criteria (ground truth only)\n",
    "\n",
    "Create a more complex evaluation report, starting with the human-annotated ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db4026-14ca-46ab-9eeb-b1f503f0c41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of metric schemas to capture the ground truth inputs. Later, you will append a custom metric schema\n",
    "from snorkelflow.client.utils import get_dataset_uid\n",
    "from snorkelflow.client_v3 import evaluation\n",
    "from snorkelflow.client_v3.evaluation import MetricSchema\n",
    "from snorkelflow.evaluation.metric_schema import GTAcceptanceRateMetricSchema\n",
    "\n",
    "\n",
    "# Use the previously-defined criteria. For each criteria, collect the label_schema_uid\n",
    "def get_label_schema_uids(dataset, criteria):\n",
    "    schema_uids = []\n",
    "    for label_schema in dataset.label_schemas:\n",
    "        if label_schema.name in criteria:\n",
    "            schema_uids.append(label_schema.uid)\n",
    "    return schema_uids\n",
    "\n",
    "\n",
    "dataset_uid = int(sf.get_dataset_uid(app_name))\n",
    "eval_dataset = sf_sdk.Dataset.get(app_name)\n",
    "\n",
    "\n",
    "criteria_names = [c[\"name\"] for c in criteria]\n",
    "criteria_uids = get_label_schema_uids(dataset=eval_dataset, criteria=criteria_names)\n",
    "\n",
    "# Define criteria based on ground truth\n",
    "ground_truth_metric_schemas = [\n",
    "    GTAcceptanceRateMetricSchema(\n",
    "        display_name=\"GT | \" + criteria[i][\"name\"],\n",
    "        dataset_uid=dataset_uid,\n",
    "        label_schema_uid=label_uid,\n",
    "        accept_label_value=1,\n",
    "    )\n",
    "    for i, label_uid in enumerate(criteria_uids)\n",
    "]\n",
    "for gt_metric_schema in ground_truth_metric_schemas:\n",
    "    ft_app.register_metric(gt_metric_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf78370-2e84-4a94-b164-a23d7f9d73ae",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26835e0a-33f7-429b-ba0d-2a914e0735d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation report based on the ground truth\n",
    "ft_app.create_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb0aab-6e54-4054-9c31-3042407a0784",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+0.64s No valid data found to calculate the metric GT | Polite Tone.\n",
    "+1.33s Found and loaded datasources for models  in dataset XXXX test split.\n",
    "+2.02s (100%) Metric compute completed. Evaluation report created with UID XXXX.\n",
    "\n",
    "{'dataset_uid': XXXX,\n",
    "\n",
    "...\n",
    "\n",
    "     'global': 100.0}}},\n",
    "  'test': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1effd-e652-480c-8052-1e2ffaaee9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the report\n",
    "eval_report_uid = get_latest_report_uid(dataset_uid)\n",
    "update_report_description(dataset_uid, eval_report_uid, \"Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553c5d1-e7f7-495c-a257-b35d1ad6d19f",
   "metadata": {},
   "source": [
    "## Create a complex evaluation report using ground truth and evaluator criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374df1d-11b4-4d1a-8444-1cd0e763f71d",
   "metadata": {},
   "source": [
    "To create a more advanced version of the evaluation report, use Snorkel Flow's `evaluation` module. The `evaluation` module allows you to combine all of collected ground truth at the dataset level with all of your custom evaluators.\n",
    "\n",
    "To combine the ground-truth based measurements with custom evaluators, serialize the evaluators defined above and pass them to the `CustomMetricSchema` class. Finally, create a new evaluation report with the ground truth metric schemas and the evaluator metric schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a2fa4-f0b3-4b37-b7a3-bf0f24673235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an LLM-based custom evaluator to the ground truth multi-criteria report\n",
    "import inspect\n",
    "from snorkelflow.evaluation.metric_schema import CustomMetricSchema\n",
    "from snorkelflow.serialization.code_asset import serialize_asset\n",
    "\n",
    "evaluator_metric_schemas = [LLMAJ_completeness]\n",
    "\n",
    "mentions_competitors_custom_metric_schema = CustomMetricSchema(\n",
    "    display_name=\"Heuristic | Mentions Competitors\",\n",
    "    description=\"If the response mentions competitors\",\n",
    "    serialized_custom_metric_func=serialize_asset(mentions_competitors),\n",
    "    raw_code=inspect.getsource(mentions_competitors),\n",
    ")\n",
    "evaluator_metric_schemas.append(mentions_competitors_custom_metric_schema)\n",
    "\n",
    "for evaluator_metric_schema in evaluator_metric_schemas:\n",
    "    ft_app.register_metric(evaluator_metric_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513bcb1a-39af-47ca-9e19-b2230062ecda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the initial benchmark report\n",
    "ft_app.create_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb141f-102e-4fc4-bcf7-ec971c7f15a3",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+0.59s Found and loaded datasources for models llama3-8B-v0 in dataset 1932 train split.\n",
    "+4.02s No valid data found to calculate the metric LLMAJ | Completeness.\n",
    "\n",
    "...\n",
    "\n",
    "{'dataset_uid': XXXX,\n",
    "\n",
    "...\n",
    "\n",
    "     'global': 100.0}}},\n",
    "  'test': {}}}\n",
    "```\n",
    "\n",
    "Now you have an initial benchmark for your chatbot's response performance. You can view these initial data points in Snorkel Flow. Select your application and then select **Evaluate** to view the initial benchmark data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604fd17-5e93-4830-975c-f16149b408cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Phase 3: Refining the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574888f-c562-49a0-b20c-9f89b98e3890",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## General Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe9767-b61d-47a5-a601-f8a19895515c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "It's important to assess the accuracy and relevance of your benchmark, or your future evaluation metrics will be measured against an inaccurate standard. This section provides guidance and practical examples for refining your benchmark.\n",
    "\n",
    "Ideally, each evaluator is validated as trustworthy in the early phases of an experiment, so it can be used to expedite developing and measuring the GenAI application.\n",
    "\n",
    "You can create multiple reports for a single experiment, or generation, of your chatbot data. It's quite likely that you will create multiple reports for the first experiment as you refine the benchmark into one that you trust.\n",
    "\n",
    "#### Collecting ground truth labels, using SME annotation\n",
    "\n",
    "After running the first round of evaluators, Snorkel recommends collecting a small number of ground truth labels for each criterion to ensure the programmatic evaluators give scores similar to human scores.\n",
    "\n",
    "If you did this before registering the evaluators, it's safe to skip this phase. For example, an enterprise-specific pre-trained PII/PHI model may not require SME annotation for the use case.\n",
    "\n",
    "Snorkel recommends re-engaging domain experts throughout development for high leverage, ambiguous classes of errors, as well as in the final rounds of development as a pipeline is on its way to production.\n",
    "\n",
    "#### Improving evaluators\n",
    "\n",
    "Certain criteria may be too difficult for a single Evaluator. For example, an organization's definition of \"Correctness\" may be so broad that developers find that an Evaluator does not accurately scale SME preferences. In cases like this, Snorkel recommends one of the following:\n",
    "\n",
    "- Break down the criteria into more fine-grained definitions that can be measured by a single Evaluator.\n",
    "- Rely on high-quality annotations for that criteria during development.\n",
    "- Collect gold standard responses and create a custom evaluator to measure similarity to the collected gold standard response.\n",
    "\n",
    "### More best practices for refining the benchmark\n",
    "\n",
    "- **If most of your data isn't captured by data slices**: Consider refining or writing new slicing functions.\n",
    "- **If a high-priority data slice is under-represented in your dataset**: Consider using Snorkel's synthetic data generation modules (SDK) to augment your existing dataset. Also consider retrieving a more diverse instruction set from an existing query stream or knowledge base.\n",
    "- **If an evaluator is innaccurate**: Use the data explorer to identify key failure modes with the evaluator, and create a batch of these innaccurate predictions for an annotator to review. Once ground truth has been collected, you can scale out these measurements via a fine-tuned quality model or include these as few-shot examples in a prompt-engineered LLM-as-judge.\n",
    "- **To scale a criterion currently measured via ground truth**: From the data explorer dialog inside the evaluation dashboard, select **Go to Studio**. Use the criterion's ground truth and Snorkel's Studio interface to write labeling functions, train a specialized model for that criterion, and register it as a custom evaluator. These fine-tuned quality models can also be used elsewhere in the platform for LLM Fine-tuning and RAG tuning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21620a49-459d-4b9b-944f-f2d0b6a4d556",
   "metadata": {},
   "source": [
    "## Example: Add or refine reference prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f737bb4-1407-480f-aa59-4e3141ed17b6",
   "metadata": {},
   "source": [
    "The initial training dataset lacked significant examples of Spanish-language chatbot prompts and responses. You can add more prompts by uploading another datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591f189-5786-49a5-a9b2-a12d87619dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload existing prompts, translated into Spanish\n",
    "import pandas as pd\n",
    "\n",
    "spanish_data = pd.read_csv(\"./eval-spanish.csv\")\n",
    "spanish_questions = spanish_data.question.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b11cf3-a1bc-4ed3-9b6b-1a4023765e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Snorkel's synthetic data generation module to augment the Spanish dataset\n",
    "augmented_questions = sf.augment_data(\n",
    "    data=spanish_questions,\n",
    "    model_name=model_name,\n",
    "    runs_per_prompt=2,\n",
    ")\n",
    "augmented_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c93a8-b622-4cc9-8549-f860925ca684",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+0.59s (100%) Running inference.\n",
    "```\n",
    "\n",
    "You should see a Jupyter-generated table of Spanish chatbot questions and responses about health insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99d6ec-dfb4-4a35-a082-638f01d4fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the augmented Spanish data to the existing dataset\n",
    "ft_app.import_data(data=spanish_data, split=\"train\", source_uid=experiment1, sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb12b40-b457-46cd-a413-a53bec20eaee",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "+0.59s Starting fine tuning data ingestion\n",
    "+1.98s (100%) Data ingestion complete\n",
    "Data ingestion complete\n",
    "+0.59s Creating active datasource for op node\n",
    "+1.29s (1%) Processing data source 1 / 1: starting.\n",
    "+11.68s (1%) Processing data source 1 / 1: starting.\n",
    "+22.08s (1%) Processing data source 1 / 1: starting.\n",
    "+32.45s (1%) Processing data source 1 / 1: starting.\n",
    "+34.53s (1%) Processing labels.\n",
    "+35.22s (100%) Transactions committed.\n",
    "\n",
    "'rq-DsLAvst2_engine-tr9a_prep-and-ingest-fine-tuning-data'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f445607-1bcd-4c79-9180-a93783724728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the reapply slices helper function created earlier\n",
    "reapply_slices(ft_app.dataset_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a07920-5ca8-47cb-8bff-ef33efe58682",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Applied 'verbose_question' slice (n=11, 5.26%)\n",
    "Successfully added 11 datapoints to slice verbose_question.\n",
    "Applied 'is_spanish' slice (n=9, 4.31%)\n",
    "Successfully added 9 datapoints to slice spanish.\n",
    "Retrieving slices for dataset XXXX.\n",
    "Successfully reapplied slices to the dataset XXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef19ac3-7026-4165-a425-7fef49421f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new evaluation report for the first experiment with the additional data\n",
    "ft_app.create_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f4395-40ec-47d3-a045-8371ea9e50a9",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+0.59s Found and loaded datasources for models llama3-8B-v0 in dataset XXXX train split.\n",
    "+11.01s Found and loaded datasources for models llama3-8B-v0 in dataset XXXX train split.\n",
    "\n",
    "...\n",
    "\n",
    "{'dataset_uid': XXXX,\n",
    "\n",
    "...\n",
    "\n",
    "     'global': 100.0}}},\n",
    "  'test': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e6bb2-e8e0-47dc-92f6-741568eb9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a description to the latest report\n",
    "eval_report_uid = get_latest_report_uid(dataset_uid)\n",
    "update_report_description(dataset_uid, eval_report_uid, \"Add Spanish data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32886766-aac0-4467-8761-36cbca2e970b",
   "metadata": {},
   "source": [
    "## Example: Add ground truth labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4feb9-f36c-4d4f-bfc3-59a1d1f0ea80",
   "metadata": {},
   "source": [
    "This section walks through the process for creating a targeted batch of data that you would like to send to annotators to collect human feedback on. In this case, you create a batch for humans to assess on the \"Completeness\" criterion, so you can use that to assess the accuracy of your LLMAJ evaluator that's assessing the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae261e-528a-4cf9-986f-4d4d35f389ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ground truth for Completeness criteria to evaluate the LLMAJ | Completeness Evaluator\n",
    "specific_x_uids = (\n",
    "    []\n",
    ")  # if you want to include specific datapoints in the batch, you can include their uids here\n",
    "create_custom_batch(\n",
    "    name=\"completeness-batch\",\n",
    "    dataset_name=app_name,\n",
    "    split=\"train\",\n",
    "    experiment_name=llama3_8B_v0,\n",
    "    label_schema_names=[\"Completeness\"],\n",
    "    batch_size=100,\n",
    "    x_uids=specific_x_uids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815705e6-23fc-499d-9a87-bf8b2ce91b7f",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "Successfully created 1 batches.\n",
    "[<snorkelflow.sdk.batch.Batch at 0x7f1150723550>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61a80-b7b7-4051-a812-b8d11029315b",
   "metadata": {},
   "source": [
    "#### External input required: Annotate the batch\n",
    "\n",
    "At this point, you would engage your annotators to label the new batch of data.\n",
    "\n",
    "Snorkel's `evaluation` module combines the ground truth human metric with the LLMAJ metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609329c6-fdc2-421e-a624-097b454c1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new evaluation report for the first experiment with the updated metric for Completeness\n",
    "ft_app.create_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c74b0f-15ed-45f6-9a42-214aee8d2d0c",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+0.59s Found and loaded datasources for models llama3-8B-v0 in dataset XXXX train split.\n",
    "+11.15s Found and loaded datasources for models llama3-8B-v0 in dataset XXXX train split.\n",
    "\n",
    "...\n",
    "\n",
    "{'dataset_uid': XXXX,\n",
    "\n",
    "...\n",
    "\n",
    "     'global': 100.0}}},\n",
    "  'test': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb54d6-325a-4924-87f6-ac5afffcefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a description to the latest report\n",
    "eval_report_uid = get_latest_report_uid(dataset_uid)\n",
    "update_report_description(dataset_uid, eval_report_uid, \"Add completeness GT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8417b2b-27e9-4a1e-81c2-1454cacef95c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Phase 4: Improving your GenAI application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bcc5cc-39eb-4f0e-aede-bf259fbf371a",
   "metadata": {},
   "source": [
    "Now that you have a trustworthy benchmark, you can use a variety to techniques to improve your GenAI application.\n",
    "\n",
    "The key task at this stage is to identify classes of errors and fix them. You can do this with a variety of techniques, including:\n",
    "\n",
    "- **LLM fine tuning**: Fine tuning allows you to change the LLM's parameters to adapt its performance to your criteria. You can use Snorkel Flow to programmatically curate a high quality, diverse training dataset that’s passed to an LLM for fine-tuning. Generated responses are brought back to Snorkel Flow for response quality labeling, error analysis, and iterative development. Amazon SageMaker is one option for fine tuning, demonstrated below.\n",
    "- **RAG tuning**: Improve the chunking, embedding, or metadata in your vector database. On request, Snorkel can provide an example notebook with instructions for using Snorkel to tune a RAG system.\n",
    "- **Prompt development**: Snorkel Flow's prompt development features will be released in early 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09b83c-7689-4ccd-8081-6604e00198c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fine tune the model powering your GenAI application using Amazon SageMaker\n",
    "\n",
    "This section relies on access and authentication secrets for Amazon SageMaker, which were defined in the [user input section](#User-input) at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f794cb-cd35-422e-b0c7-f59e9937a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "from sagemaker import Session\n",
    "from snorkelflow.sdk.fine_tuning_app import ExternalModelTrainer\n",
    "from snorkelflow.types.finetuning import FinetuningProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab937cc9-c2b4-4ceb-b164-77bccb73b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to SageMaker\n",
    "AWS_ACCESS_KEY_ID = \"aws::finetuning::access_key_id\"\n",
    "AWS_SECRET_ACCESS_KEY = \"aws::finetuning::secret_access_key\"\n",
    "SAGEMAKER_EXECUTION_ROLE = \"aws::finetuning::sagemaker_execution_role\"\n",
    "FINETUNING_AWS_REGION = \"aws::finetuning::region\"\n",
    "sf.set_secret(\n",
    "    AWS_ACCESS_KEY_ID,\n",
    "    AWS_ACCESS_KEY_ID_SECRET,\n",
    "    secret_store=\"local_store\",\n",
    "    workspace_uid=workspace_uid,\n",
    "    kwargs=None,\n",
    ")\n",
    "sf.set_secret(\n",
    "    AWS_SECRET_ACCESS_KEY,\n",
    "    AWS_SECRET_ACCESS_KEY_SECRET,\n",
    "    secret_store=\"local_store\",\n",
    "    workspace_uid=workspace_uid,\n",
    "    kwargs=None,\n",
    ")\n",
    "sf.set_secret(\n",
    "    SAGEMAKER_EXECUTION_ROLE,\n",
    "    SAGEMAKER_EXECUTION_ROLE_SECRET,\n",
    "    secret_store=\"local_store\",\n",
    "    workspace_uid=workspace_uid,\n",
    "    kwargs=None,\n",
    ")\n",
    "sf.set_secret(\n",
    "    FINETUNING_AWS_REGION,\n",
    "    \"us-west-2\",\n",
    "    secret_store=\"local_store\",\n",
    "    workspace_uid=workspace_uid,\n",
    "    kwargs=None,\n",
    ")\n",
    "boto_session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID_SECRET,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY_SECRET,\n",
    "    region_name=\"us-west-2\",\n",
    ")\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=sagemaker_runtime_client,\n",
    ")\n",
    "\n",
    "# Configurations for the fine-tuning job\n",
    "finetuning_configs = {\n",
    "    \"epoch\": \"1\",\n",
    "    \"instruction_tuned\": \"True\",\n",
    "    \"validation_split_ratio\": \"0.1\",\n",
    "    \"max_input_length\": \"1024\",\n",
    "    \"chat_dataset\": \"False\",\n",
    "}\n",
    "training_configs = {\n",
    "    # g5.12xlarge is faster but not currently available in us-west-2\n",
    "    # \"instance_type\": \"ml.g5.12xlarge\",\n",
    "    # use the below one for concurrency testing, its cheap and slow\n",
    "    # \"instance_type\": \"ml.g4dn.12xlarge\"\n",
    "    # if you want faster loop time, use the below instance type, we only have\n",
    "    # quota of 1 instance for this in us-west-2\n",
    "    \"instance_type\": \"ml.p3dn.24xlarge\"\n",
    "}\n",
    "\n",
    "external_model_trainer = ExternalModelTrainer(\n",
    "    column_mappings=cm, finetuning_provider_type=FinetuningProvider.AWS_SAGEMAKER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3ddc4-5ac8-41e6-abf6-e57fc949a52d",
   "metadata": {},
   "source": [
    "#### Curate a fine tuning dataset\n",
    "\n",
    "You can create a dataset for fine tuning your GenAI application using several methods.\n",
    "\n",
    "This notebook includes an example that gets high-quality data points from the Spanish-language data slice for your fine tuning dataset. The goal for this fine tuning is to achieve better chatbot responses to Spanish-language questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b090c-7960-4df2-8f0b-ce325fbf7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a curated dataset from good examples from the Spanish-language slice\n",
    "spanish_slice = Slice.get(dataset=app_name, slice=\"spanish\")\n",
    "df = sf.nodes.get_dataset(\n",
    "    node=ft_app.model_node_uid,\n",
    "    gt_label=\"ACCEPT\",\n",
    "    include_slice_uids=[spanish_slice.slice_uid],\n",
    ")\n",
    "good_slice_x_uids = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941954a7-e7d5-4908-97e6-3fbe76a9f723",
   "metadata": {},
   "source": [
    "Another method of curating a dataset for fine tuning is to use a quality model to select the highest-quality data points from the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0179b38-b0e7-4cf8-b4e0-cc2a75a048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the dataset creation job\n",
    "# # Create a curated dataset from a trained quality model\n",
    "# # You must train a quality model in Snorkel Flow before running this cell\n",
    "\n",
    "# qd = ft_app.get_quality_dataset(1) # Use the UID for your end model\n",
    "# qd_filtered = qd.filter(confidence_threshold=0.9, labels=[\"ACCEPT\"])\n",
    "# good_x_uids = list(qd_filtered.get_data().index)\n",
    "# good_datasource_uids = list(qd_filtered.get_data()['datasource_uid'].unique())\n",
    "# good_datasource_uids = [int(i) for i in good_datasource_uids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbd0a5-0e5f-43a6-a9ab-724ac3e4825f",
   "metadata": {},
   "source": [
    "#### Run the fine tuning job\n",
    "\n",
    "Run the cell below when you are ready to kick off the fine tuning job. For the purposes of this example notebook, instead of running your own fine-tuning job, deploying the fine-tuned model for your GenAI application, and collecting more response data, you can use an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7f515-0621-4514-a050-0b12881a52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the fine tuning job\n",
    "# external_model = external_model_trainer.finetune(\n",
    "#     base_model_id=\"meta-textgeneration-llama-3-8b-instruct\",\n",
    "#     base_model_version=\"2.*\",\n",
    "#     finetuning_configs=finetuning_configs,\n",
    "#     training_configs=training_configs,\n",
    "#     datasource_uids= good_datasource_uids,\n",
    "#     # to filter on x_uids, uncomment here\n",
    "#     x_uids=good_x_uids,\n",
    "#     # Set sync=False to return a job id and release the notebook kernel\n",
    "#     sync=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7153f6-8b9e-4705-899b-d4228fca40fe",
   "metadata": {},
   "source": [
    "### Add data from the second experiment to Snorkel Flow\n",
    "\n",
    "This section shows how to add data from a new generation of your GenAI application to Snorkel Flow so you can create the next evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606eae98-e655-4244-89a2-3176a5fd296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data from the second generation of your GenAI app\n",
    "fine_tuned_data = pd.read_csv(\"./eval-train-2.csv\")\n",
    "\n",
    "# Create a new experiment in Snorkel to track results\n",
    "# This creates a new checkpoint for the evaluation graph\n",
    "llama3_8B_v1 = \"llama3-8B-v1\"\n",
    "experiment2 = ft_app.register_model_source(\n",
    "    llama3_8B_v1, metadata=ModelSourceMetadata(model_name=llama3_8B_v1)\n",
    ")[\"source_uid\"]\n",
    "\n",
    "ft_app.import_data(\n",
    "    data=fine_tuned_data, split=\"train\", source_uid=experiment2, sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21b13a-52ae-4887-af50-1c126653451f",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Successfully retrieved dataset evaluation-example-app with UID XXXX.\n",
    "+0.59s Starting fine tuning data ingestion\n",
    "+1.99s (100%) Data ingestion complete\n",
    "Data ingestion complete\n",
    "+0.59s Creating active datasource for op node\n",
    "+1.29s (1%) Processing data source 1 / 1: starting.\n",
    "+11.71s (1%) Processing data source 1 / 1: starting.\n",
    "+21.71s (1%) Processing data source 1 / 1: starting.\n",
    "+32.07s (1%) Processing data source 1 / 1: starting.\n",
    "+42.47s (1%) Processing data source 1 / 1: starting.\n",
    "+52.90s (1%) Processing data source 1 / 1: starting.\n",
    "+63.44s (1%) Processing data source 1 / 1: starting.\n",
    "+69.66s (90%) Active datasources preprocessing complete. Committing transactions. This can take several minutes.\n",
    "+70.35s (100%) Transactions committed.\n",
    "\n",
    "'rq-UezNqGLj_engine-u428_prep-and-ingest-fine-tuning-data'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f5bab-dad0-4d08-a518-81a7caeed5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply the slices\n",
    "reapply_slices(ft_app.dataset_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df215877-6e9a-4096-bd1a-68fdcc6e422f",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Applied 'verbose_question' slice (n=13, 4.21%)\n",
    "Successfully added 13 datapoints to slice verbose_question.\n",
    "Applied 'is_spanish' slice (n=9, 2.91%)\n",
    "Successfully added 9 datapoints to slice spanish.\n",
    "Retrieving slices for dataset XXXX.\n",
    "Successfully reapplied slices to the dataset XXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eba49e-93b8-4a94-9d6e-3246fad707ab",
   "metadata": {},
   "source": [
    "### Evaluate the second generation of GenAI responses\n",
    "\n",
    "Create a new evaluation report with the new experiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733d4ba-a212-45b6-8adf-f7c27953da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation report for the second experiment\n",
    "ft_app.create_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb8054-1a65-42ab-ad19-734f7bd3b4b8",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+0.59s Found and loaded datasources for models llama3-8B-v0, llama3-8B-v1 in dataset XXXX train split.\n",
    "+10.67s Found and loaded datasources for models llama3-8B-v0, llama3-8B-v1 in dataset XXXX train split.\n",
    "\n",
    "...\n",
    "\n",
    "{'dataset_uid': 1932,\n",
    "\n",
    "...\n",
    "\n",
    "     'global': 100.0}}},\n",
    "  'test': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dacfea-6678-40ee-a924-88fbf7ac1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a description to the latest report\n",
    "eval_report_uid = get_latest_report_uid(dataset_uid)\n",
    "update_report_description(dataset_uid, eval_report_uid, \"Fine tuning iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389b267-fadb-48de-8832-0408bf2191d1",
   "metadata": {},
   "source": [
    "Revisit the **Evaluate** page for your app in the Snorkel Flow app.\n",
    "\n",
    "Now you should see two evaluation scores for each of your criteria, allowing you to visually identify trends in performance.\n",
    "\n",
    "You can also toggle the option to **Compare models** to view performance deltas across experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a957b52-ea40-4d85-8fee-5d42f2bd694b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Keep iterating on your GenAI application until its performance across the evaluation criteria meets your standards for production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
