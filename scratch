from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import numpy as np
import base64
import aiohttp

router = APIRouter()

class EmbeddingRequest(BaseModel):
    input: str | List[str]
    model: str
    encoding_format: Optional[str] = "float"  # Supports "float" or "base64"

class EmbeddingResponse(BaseModel):
    object: str = "list"
    data: List[dict]
    model: str
    usage: dict

async def get_embeddings(request: EmbeddingRequest):
    try:
        # Convert single string input to list for consistent processing
        inputs = [request.input] if isinstance(request.input, str) else request.input
        
        # Format request according to Triton HTTP API
        inference_payload = {
            "inputs": [
                {
                    "name": "text_input",
                    "shape": [len(inputs)],
                    "datatype": "BYTES",
                    "data": inputs
                }
            ]
        }
        
        # Make direct HTTP request to model endpoint
        async with aiohttp.ClientSession() as session:
            url = f"http://localhost:9000/v1/models/{request.model}"
            async with session.post(url, json=inference_payload) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise HTTPException(
                        status_code=response.status, 
                        detail=f"Triton error: {error_text}"
                    )
                
                result = await response.json()
                # Extract embeddings from the response
                embeddings = np.array(result["outputs"][0]["data"]).reshape(result["outputs"][0]["shape"])
                
                # Format the response as expected by OpenAI compatible API
                data = []
                for i, emb in enumerate(embeddings):
                    if request.encoding_format == "base64":
                        emb_bytes = np.array(emb, dtype=np.float32).tobytes()
                        emb_encoded = base64.b64encode(emb_bytes).decode("utf-8")
                        embedding_data = emb_encoded
                    else:
                        embedding_data = emb.tolist() if isinstance(emb, np.ndarray) else emb
                    data.append({
                        "object": "embedding", 
                        "embedding": embedding_data, 
                        "index": i
                    })
                
                return EmbeddingResponse(
                    data=data,
                    model=request.model,
                    usage={"prompt_tokens": len(inputs), "total_tokens": len(inputs)}
                )
                
    except aiohttp.ClientError as e:
        raise HTTPException(status_code=500, detail=f"HTTP request failed: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Embedding inference failed: {str(e)}")

@router.post("/v1/embeddings")
async def embeddings_endpoint(request: EmbeddingRequest):
    return await get_embeddings(request)
