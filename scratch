embedding.py -----

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import numpy as np
import tritonclient.http as triton_http

router = APIRouter()

class EmbeddingRequest(BaseModel):
    input: str | List[str]
    model: str
    encoding_format: Optional[str] = "float"

class EmbeddingResponse(BaseModel):
    object: str = "list"
    data: List[dict]
    model: str
    usage: dict

async def get_embeddings(request: EmbeddingRequest):
    try:
        client = triton_http.InferenceServerClient(url="localhost:8000")
        inputs = [request.input] if isinstance(request.input, str) else request.input
        
        input_data = np.array(inputs, dtype=object)
        triton_input = triton_http.InferInput("text_input", input_data.shape, "BYTES")
        triton_input.set_data_from_numpy(input_data)

        result = client.infer(model_name=request.model, inputs=[triton_input])
        embeddings = result.as_numpy("embeddings")

        data = [{"object": "embedding", "embedding": emb.tolist(), "index": i} for i, emb in enumerate(embeddings)]
        return EmbeddingResponse(
            data=data,
            model=request.model,
            usage={"prompt_tokens": len(inputs), "total_tokens": len(inputs)}
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Embedding inference failed: {str(e)}")

@router.post("/v1/embeddings")
async def embeddings_endpoint(request: EmbeddingRequest):
    return await get_embeddings(request)

------
main.py 

from openai_frontend.embedding import router as embedding_router  # Add this import

app.include_router(embedding_router, prefix="")  # Add this line
