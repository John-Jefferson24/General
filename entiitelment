#!/usr/bin/env python3
"""
Triton Endpoint Performance Tester
Tests OpenAI-compatible endpoints and collects comprehensive metrics
"""

import time
import json
import statistics
from dataclasses import dataclass
from typing import List, Dict, Optional
import requests
import asyncio
import aiohttp
from datetime import datetime

@dataclass
class TestMetrics:
    """Container for test metrics"""
    total_time: float
    time_to_first_token: float
    tokens_per_second: float
    total_tokens: int
    prompt_tokens: int
    completion_tokens: int
    request_latency: float
    response_size: int
    status_code: int
    error: Optional[str] = None

class TritonTester:
    def __init__(self, base_url: str, api_key: str = "dummy", model: str = "model"):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def test_sync_completion(self, prompt: str, max_tokens: int = 100) -> TestMetrics:
        """Test synchronous completion endpoint"""
        url = f"{self.base_url}/v1/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": 0.7,
            "stream": False
        }
        
        start_time = time.time()
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            end_time = time.time()
            
            total_time = end_time - start_time
            
            if response.status_code == 200:
                data = response.json()
                usage = data.get("usage", {})
                
                return TestMetrics(
                    total_time=total_time,
                    time_to_first_token=total_time,  # For non-streaming
                    tokens_per_second=usage.get("completion_tokens", 0) / total_time if total_time > 0 else 0,
                    total_tokens=usage.get("total_tokens", 0),
                    prompt_tokens=usage.get("prompt_tokens", 0),
                    completion_tokens=usage.get("completion_tokens", 0),
                    request_latency=total_time,
                    response_size=len(response.content),
                    status_code=response.status_code
                )
            else:
                return TestMetrics(
                    total_time=total_time,
                    time_to_first_token=0,
                    tokens_per_second=0,
                    total_tokens=0,
                    prompt_tokens=0,
                    completion_tokens=0,
                    request_latency=total_time,
                    response_size=len(response.content),
                    status_code=response.status_code,
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except Exception as e:
            return TestMetrics(
                total_time=time.time() - start_time,
                time_to_first_token=0,
                tokens_per_second=0,
                total_tokens=0,
                prompt_tokens=0,
                completion_tokens=0,
                request_latency=0,
                response_size=0,
                status_code=0,
                error=str(e)
            )

    def test_streaming_completion(self, prompt: str, max_tokens: int = 100) -> TestMetrics:
        """Test streaming completion endpoint"""
        url = f"{self.base_url}/v1/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": 0.7,
            "stream": True
        }
        
        start_time = time.time()
        first_token_time = None
        total_tokens = 0
        completion_tokens = 0
        response_size = 0
        
        try:
            response = requests.post(url, headers=headers, json=payload, stream=True, timeout=30)
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        response_size += len(line)
                        if first_token_time is None:
                            first_token_time = time.time()
                        
                        line_str = line.decode('utf-8')
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]
                            if data_str.strip() == '[DONE]':
                                break
                            
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and data['choices']:
                                    completion_tokens += 1
                                    total_tokens += 1
                            except json.JSONDecodeError:
                                continue
                
                end_time = time.time()
                total_time = end_time - start_time
                time_to_first_token = first_token_time - start_time if first_token_time else 0
                
                return TestMetrics(
                    total_time=total_time,
                    time_to_first_token=time_to_first_token,
                    tokens_per_second=completion_tokens / total_time if total_time > 0 else 0,
                    total_tokens=total_tokens,
                    prompt_tokens=total_tokens - completion_tokens,
                    completion_tokens=completion_tokens,
                    request_latency=time_to_first_token,
                    response_size=response_size,
                    status_code=response.status_code
                )
            else:
                return TestMetrics(
                    total_time=time.time() - start_time,
                    time_to_first_token=0,
                    tokens_per_second=0,
                    total_tokens=0,
                    prompt_tokens=0,
                    completion_tokens=0,
                    request_latency=time.time() - start_time,
                    response_size=len(response.content),
                    status_code=response.status_code,
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except Exception as e:
            return TestMetrics(
                total_time=time.time() - start_time,
                time_to_first_token=0,
                tokens_per_second=0,
                total_tokens=0,
                prompt_tokens=0,
                completion_tokens=0,
                request_latency=0,
                response_size=0,
                status_code=0,
                error=str(e)
            )

    async def test_async_completion(self, prompt: str, max_tokens: int = 100) -> TestMetrics:
        """Test asynchronous completion endpoint"""
        url = f"{self.base_url}/v1/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": 0.7,
            "stream": False
        }
        
        start_time = time.time()
        
        try:
            async with self.session.post(url, headers=headers, json=payload, timeout=30) as response:
                end_time = time.time()
                content = await response.read()
                
                total_time = end_time - start_time
                
                if response.status == 200:
                    data = json.loads(content)
                    usage = data.get("usage", {})
                    
                    return TestMetrics(
                        total_time=total_time,
                        time_to_first_token=total_time,
                        tokens_per_second=usage.get("completion_tokens", 0) / total_time if total_time > 0 else 0,
                        total_tokens=usage.get("total_tokens", 0),
                        prompt_tokens=usage.get("prompt_tokens", 0),
                        completion_tokens=usage.get("completion_tokens", 0),
                        request_latency=total_time,
                        response_size=len(content),
                        status_code=response.status
                    )
                else:
                    return TestMetrics(
                        total_time=total_time,
                        time_to_first_token=0,
                        tokens_per_second=0,
                        total_tokens=0,
                        prompt_tokens=0,
                        completion_tokens=0,
                        request_latency=total_time,
                        response_size=len(content),
                        status_code=response.status,
                        error=f"HTTP {response.status}: {content.decode()}"
                    )
                    
        except Exception as e:
            return TestMetrics(
                total_time=time.time() - start_time,
                time_to_first_token=0,
                tokens_per_second=0,
                total_tokens=0,
                prompt_tokens=0,
                completion_tokens=0,
                request_latency=0,
                response_size=0,
                status_code=0,
                error=str(e)
            )

    def test_embeddings(self, text: str) -> TestMetrics:
        """Test embeddings endpoint"""
        url = f"{self.base_url}/v1/embeddings"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "input": text
        }
        
        start_time = time.time()
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            end_time = time.time()
            
            total_time = end_time - start_time
            
            if response.status_code == 200:
                data = response.json()
                usage = data.get("usage", {})
                embeddings = data.get("data", [])
                
                return TestMetrics(
                    total_time=total_time,
                    time_to_first_token=total_time,  # For embeddings, this is just response time
                    tokens_per_second=usage.get("total_tokens", 0) / total_time if total_time > 0 else 0,
                    total_tokens=usage.get("total_tokens", 0),
                    prompt_tokens=usage.get("prompt_tokens", 0),
                    completion_tokens=len(embeddings),  # Number of embedding vectors
                    request_latency=total_time,
                    response_size=len(response.content),
                    status_code=response.status_code
                )
            else:
                return TestMetrics(
                    total_time=total_time,
                    time_to_first_token=0,
                    tokens_per_second=0,
                    total_tokens=0,
                    prompt_tokens=0,
                    completion_tokens=0,
                    request_latency=total_time,
                    response_size=len(response.content),
                    status_code=response.status_code,
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except Exception as e:
            return TestMetrics(
                total_time=time.time() - start_time,
                time_to_first_token=0,
                tokens_per_second=0,
                total_tokens=0,
                prompt_tokens=0,
                completion_tokens=0,
                request_latency=0,
                response_size=0,
                status_code=0,
                error=str(e)
            )

    async def test_async_embeddings(self, text: str) -> TestMetrics:
        """Test asynchronous embeddings endpoint"""
        url = f"{self.base_url}/v1/embeddings"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "input": text
        }
        
        start_time = time.time()
        
        try:
            async with self.session.post(url, headers=headers, json=payload, timeout=30) as response:
                end_time = time.time()
                content = await response.read()
                
                total_time = end_time - start_time
                
                if response.status == 200:
                    data = json.loads(content)
                    usage = data.get("usage", {})
                    embeddings = data.get("data", [])
                    
                    return TestMetrics(
                        total_time=total_time,
                        time_to_first_token=total_time,
                        tokens_per_second=usage.get("total_tokens", 0) / total_time if total_time > 0 else 0,
                        total_tokens=usage.get("total_tokens", 0),
                        prompt_tokens=usage.get("prompt_tokens", 0),
                        completion_tokens=len(embeddings),
                        request_latency=total_time,
                        response_size=len(content),
                        status_code=response.status
                    )
                else:
                    return TestMetrics(
                        total_time=total_time,
                        time_to_first_token=0,
                        tokens_per_second=0,
                        total_tokens=0,
                        prompt_tokens=0,
                        completion_tokens=0,
                        request_latency=total_time,
                        response_size=len(content),
                        status_code=response.status,
                        error=f"HTTP {response.status}: {content.decode()}"
                    )
                    
        except Exception as e:
            return TestMetrics(
                total_time=time.time() - start_time,
                time_to_first_token=0,
                tokens_per_second=0,
                total_tokens=0,
                prompt_tokens=0,
                completion_tokens=0,
                request_latency=0,
                response_size=0,
                status_code=0,
                error=str(e)
            )

def print_metrics(metrics: TestMetrics, test_name: str):
    """Print metrics in a formatted way"""
    print(f"\n{'='*50}")
    print(f"Test: {test_name}")
    print(f"{'='*50}")
    
    if metrics.error:
        print(f"âŒ Error: {metrics.error}")
        return
    
    print(f"âœ… Status Code: {metrics.status_code}")
    print(f"ðŸ• Total Time: {metrics.total_time:.3f}s")
    print(f"âš¡ Time to First Token: {metrics.time_to_first_token:.3f}s")
    print(f"ðŸš€ Tokens/Second: {metrics.tokens_per_second:.2f}")
    print(f"ðŸ“ Total Tokens: {metrics.total_tokens}")
    print(f"ðŸ“¥ Prompt Tokens: {metrics.prompt_tokens}")
    print(f"ðŸ“¤ Completion Tokens: {metrics.completion_tokens}")
    print(f"ðŸŒ Request Latency: {metrics.request_latency:.3f}s")
    print(f"ðŸ“Š Response Size: {metrics.response_size} bytes")

def calculate_aggregate_metrics(metrics_list: List[TestMetrics]) -> Dict:
    """Calculate aggregate metrics from multiple runs"""
    successful_metrics = [m for m in metrics_list if not m.error]
    
    if not successful_metrics:
        return {"error": "No successful requests"}
    
    return {
        "total_requests": len(metrics_list),
        "successful_requests": len(successful_metrics),
        "error_rate": (len(metrics_list) - len(successful_metrics)) / len(metrics_list) * 100,
        "avg_total_time": statistics.mean(m.total_time for m in successful_metrics),
        "avg_time_to_first_token": statistics.mean(m.time_to_first_token for m in successful_metrics),
        "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
        "avg_total_tokens": statistics.mean(m.total_tokens for m in successful_metrics),
        "avg_request_latency": statistics.mean(m.request_latency for m in successful_metrics),
        "p95_total_time": statistics.quantiles(sorted(m.total_time for m in successful_metrics), n=20)[18] if len(successful_metrics) >= 20 else max(m.total_time for m in successful_metrics),
        "p95_time_to_first_token": statistics.quantiles(sorted(m.time_to_first_token for m in successful_metrics), n=20)[18] if len(successful_metrics) >= 20 else max(m.time_to_first_token for m in successful_metrics),
    }

async def run_load_test(tester: TritonTester, prompt: str, num_concurrent: int = 5, num_requests: int = 10) -> List[TestMetrics]:
    """Run concurrent load test"""
    print(f"Running load test: {num_concurrent} concurrent requests, {num_requests} total requests")
    
    metrics = []
    semaphore = asyncio.Semaphore(num_concurrent)
    
    async def limited_request():
        async with semaphore:
            return await tester.test_async_completion(prompt)
    
    tasks = [limited_request() for _ in range(num_requests)]
    results = await asyncio.gather(*tasks)
    
    return results

async def main():
    """Main test function"""
    # Configuration
    ENDPOINT_URL = "http://localhost:8000"  # Change to your Triton endpoint
    API_KEY = "dummy"  # Change if authentication is required
    MODEL_NAME = "model"  # Change to your model name
    
    TEST_PROMPT = "Write a short story about a robot learning to paint."
    EMBEDDING_TEXT = "This is a test sentence for embedding generation."
    MAX_TOKENS = 50
    
    print("ðŸš€ Starting Triton Endpoint Performance Test")
    print(f"Endpoint: {ENDPOINT_URL}")
    print(f"Model: {MODEL_NAME}")
    print(f"Test Prompt: {TEST_PROMPT[:50]}...")
    
    tester = TritonTester(ENDPOINT_URL, API_KEY, MODEL_NAME)
    
    # Test 1: Synchronous completion
    print("\nðŸ“Š Test 1: Synchronous Completion")
    sync_metrics = tester.test_sync_completion(TEST_PROMPT, MAX_TOKENS)
    print_metrics(sync_metrics, "Synchronous Completion")
    
    # Test 2: Streaming completion
    print("\nðŸ“Š Test 2: Streaming Completion")
    stream_metrics = tester.test_streaming_completion(TEST_PROMPT, MAX_TOKENS)
    print_metrics(stream_metrics, "Streaming Completion")
    
    # Test 3: Embeddings
    print("\nðŸ“Š Test 3: Embeddings")
    embedding_metrics = tester.test_embeddings(EMBEDDING_TEXT)
    print_metrics(embedding_metrics, "Embeddings")
    
    # Test 4: Asynchronous completion
    print("\nðŸ“Š Test 4: Asynchronous Completion")
    async with tester:
        async_metrics = await tester.test_async_completion(TEST_PROMPT, MAX_TOKENS)
        print_metrics(async_metrics, "Asynchronous Completion")
        
        # Test 5: Asynchronous embeddings
        print("\nðŸ“Š Test 5: Asynchronous Embeddings")
        async_embedding_metrics = await tester.test_async_embeddings(EMBEDDING_TEXT)
        print_metrics(async_embedding_metrics, "Asynchronous Embeddings")
    
    # Test 6: Load test (completions)
    print("\nðŸ“Š Test 6: Load Test (Completions)")
    async with tester:
        load_metrics = await run_load_test(tester, TEST_PROMPT, num_concurrent=3, num_requests=10)
    
    # Calculate aggregate metrics
    aggregate = calculate_aggregate_metrics(load_metrics)
    
    print(f"\n{'='*50}")
    print("Load Test Results (Completions)")
    print(f"{'='*50}")
    print(f"Total Requests: {aggregate['total_requests']}")
    print(f"Successful Requests: {aggregate['successful_requests']}")
    print(f"Error Rate: {aggregate['error_rate']:.2f}%")
    print(f"Average Total Time: {aggregate['avg_total_time']:.3f}s")
    print(f"Average Time to First Token: {aggregate['avg_time_to_first_token']:.3f}s")
    print(f"Average Tokens/Second: {aggregate['avg_tokens_per_second']:.2f}")
    print(f"Average Total Tokens: {aggregate['avg_total_tokens']:.0f}")
    print(f"P95 Total Time: {aggregate['p95_total_time']:.3f}s")
    print(f"P95 Time to First Token: {aggregate['p95_time_to_first_token']:.3f}s")
    
    # Export results to JSON
    results = {
        "timestamp": datetime.now().isoformat(),
        "endpoint": ENDPOINT_URL,
        "model": MODEL_NAME,
        "sync_completion_test": sync_metrics.__dict__,
        "stream_completion_test": stream_metrics.__dict__,
        "embeddings_test": embedding_metrics.__dict__,
        "async_completion_test": async_metrics.__dict__,
        "async_embeddings_test": async_embedding_metrics.__dict__,
        "load_test": {
            "individual_results": [m.__dict__ for m in load_metrics],
            "aggregate_metrics": aggregate
        }
    }
    
    with open(f"triton_perf_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… Test completed! Results saved to JSON file.")

if __name__ == "__main__":
    asyncio.run(main())
