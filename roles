livenessProbe:
  httpGet:
    path: /v2/health/ready
    port: 9000
  initialDelaySeconds: 10
  periodSeconds: 20
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /v2/health/live
    port: 9000
  initialDelaySeconds: 15
  periodSeconds: 10
  failureThreshold: 3

llama
    livenessProbe:
      httpGet:
        path: /health/ready
        port: 9000
      initialDelaySeconds: 480 
      periodSeconds: 30 
      timeoutSeconds: 10
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /health/ready
        port: 9000
      initialDelaySeconds: 420
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 12

apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: triton-openai-route
  annotations:
    # Health check configuration
    haproxy.router.openshift.io/health_check_interval: "10s"
    haproxy.router.openshift.io/health_check_uri: "/health/ready"
    
    # Traffic management
    haproxy.router.openshift.io/balance: "roundrobin"
    haproxy.router.openshift.io/disable_cookies: "true"
    
    # Optional: Timeout settings
    haproxy.router.openshift.io/timeout: "60s"
    haproxy.router.openshift.io/timeout-tunnel: "1h"
    
    # Optional: Health check timeout
    haproxy.router.openshift.io/health_check_timeout: "5s"
spec:
  host: api.bank.com
  to:
    kind: Service
    name: triton-openai-service
    weight: 100
  port:
    targetPort: 9000
  tls:
    termination: edge  # or passthrough/reencrypt as needed


Hi team,
Quick update on the routing and health check changes I've completed:
What's been done:

LLaMA 70B model: Set up health checks on port 9000 using /health/ready
Jina embedding model: Configured two health endpoints - /v2/health/ready and /v2/health/alive
All necessary route changes have been applied
Added route-level health checks with the following annotations:
haproxy.router.openshift.io/health_check_interval: "10s"
haproxy.router.openshift.io/health_check_uri: "/health/ready" # or appropriate endpoint


How these health checks work:
Pod-level health checks:

Readiness probe (/health/ready or /v2/health/ready): Determines if a pod is ready to receive traffic. If this fails, the pod is removed from service endpoints but keeps running
Liveness probe (/v2/health/alive): Checks if the pod is still functioning. If this fails, Kubernetes restarts the pod
These ensure we don't send traffic to pods that aren't ready or are stuck/crashed

Route-level health checks (HAProxy):

The HAProxy router performs its own health checks against backend pods
Every 10 seconds, it hits the specified health endpoint on each pod
If a pod fails these checks, HAProxy stops routing traffic to it (even if Kubernetes still thinks it's ready)
This provides an additional layer of protection at the ingress level

Next steps - Repository updates:
We need to add these configurations to our repo to ensure they're properly tracked and deployed:

Update deployment manifests with the new health check endpoints
Add routing configurations for both models (including the HAProxy annotations)
This will ensure consistency across all environments and make future deployments smoother

Question about the Wide IP health check requirement:
There was also a request for Wide IP health checks, and I wanted to clarify my understanding here. From what I know, the Wide IP (api.bank.com) is managed at the GSLB/network level and handles global load balancing between data centers.
My understanding is that we can't directly configure Wide IP health checks from our platform side - this would typically be handled by the network team who manages the GSLB. What we can do is ensure our ingress health endpoints are available for the GSLB to monitor.
Am I thinking about this correctly? If there's a specific Wide IP health check requirement I should be implementing differently, please let me know and we can discuss the best approach.
Let me know if you have any questions or if anything needs adjustment.
